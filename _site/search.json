[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nothing Plain About It",
    "section": "",
    "text": "Modeling Airports\n\n\n\n\n\n\n\nFlights\n\n\nR\n\n\n\n\nBegin simmulating passenger and bag traffic\n\n\n\n\n\n\nJun 1, 2024\n\n\nRyan Plain\n\n\n\n\n\n\n  \n\n\n\n\nArrow, Python, & R\n\n\n\n\n\n\n\nArrow\n\n\nPython\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2023\n\n\nRyan Plain\n\n\n\n\n\n\n  \n\n\n\n\nBlank Canvas\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2023\n\n\nRyan Plain\n\n\n\n\n\n\n  \n\n\n\n\nFantasy Football Data Wrangling for Keepers\n\n\n\n\n\n\n\nFantasy Football\n\n\nAnalysis\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2021\n\n\nRyan Plain\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Blog Reconstruction",
    "section": "",
    "text": "Hi, welcome to my site!\nI recently finished grad school, moved states, and got married. Life has been hectic and not allowed time for public work. I’m hoping to be able to tackle some public projects moving forward.\nI’m particularly excited about new developments with the Apache Arrow project, and how to bridge the gap with R and Python users. I have ADHD, which means I think in a non-linear and sporadic fashion. R will always be my true love for data analysis, with non-standard evaluation and emphasis by the Posit team on designing their API’s to be a fluid conversation with data.\nI have love for python too. Python was the first language I picked up. It taught me the beauty of understanding another way of thinking (something I again rediscovered learning about octopuses). I never excelled in math in the traditional school setting, but being able to define functions and interactively test different inputs really connected me in a way that greek letters and chalkboard never could. This is especially true for linear algebra. I don’t know that I would have any understanding if it wasn’t for numpy.\nThis is why I’m excited about polars. I love numpy and the optimization and syntax, however, it did not hold up well as a pandas backend as data grew more complex. Polars offers a syntax that requires less cognitive overhead, and the backed power of arrow and rust.\nAt work, I use these tools to be able to work in the R ecosystem for data exploration, which works for me. Additionally, I’m able to leverage arrow to convert back to python for our team’s development and machine learning environment.\nMy first (new) post will be detailing what that workflow looks like, and how easy it is to work within Quarto.\nIn the meantime, I left one (1) silly post about fantasy football. Mainly because I run that script once a year and still need it. Plus it’s always neat to be able to look back and see how far you’ve come."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a data analyst living in Colorado Springs, currently working in product analytics. Previously I worked in aviation simulating airport facilities.\nMy passion is in making processes more efficient. Whether that is ETL pipelines to expose better analytics tables, visualizations that save time on investigating or filtering, or automating tedious steps in the workflow. I’m a proponent of working with what tool and environment works best for you, and enjoy designing solutions to be inclusive for everyone.\nIn my free time I love exploring the outdoors, climbing, yoga, and spending time with my wife, dogs, and evil cat."
  },
  {
    "objectID": "posts/2021-03-21-complicated-keeper-data-manipulation/complicated-keeper-data-manipulation.html",
    "href": "posts/2021-03-21-complicated-keeper-data-manipulation/complicated-keeper-data-manipulation.html",
    "title": "Complicated Keeper Data Manipulation",
    "section": "",
    "text": "My previous post dove in on sample analysis of our fantasy football league utilizing the API from Sleeper. Since then, I have discovered an amazing package {ffscrapr} on CRAN, developed by Tan. This package does everything for you to get data from Sleeper into easy to work with data frames."
  },
  {
    "objectID": "posts/2021-03-21-complicated-keeper-data-manipulation/complicated-keeper-data-manipulation.html#the-goal",
    "href": "posts/2021-03-21-complicated-keeper-data-manipulation/complicated-keeper-data-manipulation.html#the-goal",
    "title": "Complicated Keeper Data Manipulation",
    "section": "The Goal",
    "text": "The Goal\n\n\n\n\n\nEach year we draft players for the upcoming season, which is typically referred to as redraft format. A couple years ago we began a keeper format where we can keep a player from the previous year.\nThere are several ways to implement a keeper format, our rules are as follows:\n\nKeep 1 player, that player can not be kept consecutive years\n\nThe kept player will be kept at the round they were drafted in with a single round penalty (i.e. if you drafted a player in round 10, you would keep them in round 9 the following year)\n\nIf you trade a player, the new team gets the rights to keep that player\n\nThe player must remain on the roster at least until the week before their respective bye week\n\nAs you can imagine, this is a nightmare to track in a spreadsheet as a commissioner."
  },
  {
    "objectID": "posts/2021-03-21-complicated-keeper-data-manipulation/complicated-keeper-data-manipulation.html#the-solution",
    "href": "posts/2021-03-21-complicated-keeper-data-manipulation/complicated-keeper-data-manipulation.html#the-solution",
    "title": "Complicated Keeper Data Manipulation",
    "section": "The Solution",
    "text": "The Solution\nOnly a few packages needed (if you count {tidyverse} as a few that is) to begin the analysis.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
  },
  {
    "objectID": "posts/2021-03-21-complicated-keeper-data-manipulation/complicated-keeper-data-manipulation.html#ffscrapr",
    "href": "posts/2021-03-21-complicated-keeper-data-manipulation/complicated-keeper-data-manipulation.html#ffscrapr",
    "title": "Complicated Keeper Data Manipulation",
    "section": "ffscrapr",
    "text": "ffscrapr\nTan has well documented how the package works through the {ffscraper} website. I won’t go into too much detail, as the API is fairly simple to understand from the function names alone. Basically, create a league object and get the draft picks and transactions for it.\nThe league I will be focusing on is a leauge is called The Hot Boyz… Hopefully Dallas Cowboys fans understand!\n\nmy_leagues <- ffscrapr::sleeper_userleagues(\"rplain\", 2020)\n\nleague_id <- my_leagues %>% \n  filter(league_name == 'The Hot Boyz') %>% \n  pull(league_id)\n\nmy_league <- ffscrapr::ff_connect(platform = 'sleeper', \n                                  season = 2020, \n                                  league_id = league_id)\n\ntransactions <- ff_transactions(my_league)\n\ndraft_picks <- ff_draft(my_league) \n\n\nBelow is what the draft board looked like following the draft.\n\ndraft_picks %>% \n  select(round, pick, franchise_name, player_name, pos, team) %>% \n   mutate(player_name = ifelse(is.na(player_name), paste(team, \" Def\"), player_name)) %>% \n  pivot_wider(\n    id_cols = round,\n    names_from = franchise_name,\n    values_from = player_name\n  ) %>% \n  `colnames<-`(c(\"Round\",1:10)) %>% \n  gt::gt()\n\n\n\n\n\n  \n  \n    \n      Round\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n    \n  \n  \n    1\nChristian McCaffrey\nEzekiel Elliott\nSaquon Barkley\nMichael Thomas\nDalvin Cook\nAlvin Kamara\nLe'Veon Bell\nJoe Mixon\nMiles Sanders\nDavante Adams\n    2\nChris Godwin\nJosh Jacobs\nDeAndre Hopkins\nTravis Kelce\nTyreek Hill\nPatrick Mahomes\nNick Chubb\nChris Carson\nJulio Jones\nClyde Edwards-Helaire\n    3\nJames Conner\nMike Evans\nAustin Ekeler\nDavid Johnson\nGeorge Kittle\nAdam Thielen\nTodd Gurley\nKenny Golladay\nJonathan Taylor\nMark Andrews\n    4\nRobert Woods\nJuJu Smith-Schuster\nAmari Cooper\nAaron Jones\nMelvin Gordon\nCalvin Ridley\nRaheem Mostert\nAllen Robinson\nOdell Beckham\nDerrick Henry\n    5\nMark Ingram\nA.J. Brown\nKareem Hunt\nCooper Kupp\nD.J. Chark\nZach Ertz\nStefon Diggs\nKeenan Allen\nD.J. Moore\nTerry McLaurin\n    6\nMichael Gallup\nMarquise Brown\nDevin Singletary\nRussell Wilson\nDak Prescott\nKenyan Drake\nTyler Lockett\nT.Y. Hilton\nCam Akers\nDeshaun Watson\n    7\nAntonio Gibson\nJ.K. Dobbins\nDarren Waller\nDavid Montgomery\nLeonard Fournette\nDeVante Parker\nWill Fuller\nMarlon Mack\nZack Moss\nTyler Boyd\n    8\nBrandin Cooks\nRonald Jones\nA.J. Green\nScott Miller\nSF  Def\nJames White\nRob Gronkowski\nEvan Engram\nKyler Murray\nJulian Edelman\n    9\nLamar Jackson\nCeeDee Lamb\nJerry Jeudy\nJordan Howard\nDiontae Johnson\nHenry Ruggs\nTyler Higbee\nD'Andre Swift\nDeebo Samuel\nJarvis Landry\n    10\nNoah Fant\nJared Cook\nMatt Ryan\nHayden Hurst\nEmmanuel Sanders\nAustin Hooper\nCam Newton\nMarvin Jones\nTarik Cohen\nMatt Breida\n    11\nPIT  Def\nTom Brady\nJohn Brown\nPhillip Lindsay\nChristian Kirk\nBAL  Def\nNE  Def\nChris Thompson\nChase Edmonds\nJamison Crowder\n    12\nTony Pollard\nKerryon Johnson\nJustin Jefferson\nBUF  Def\nTevin Coleman\nSony Michel\nSterling Shepard\nLatavius Murray\nDamien Harris\nAaron Rodgers\n    13\nAnthony Miller\nMecole Hardman\nBlake Jarwin\nAlexander Mattison\nDarius Slayton\nJoe Burrow\nCourtland Sutton\nJosh Allen\nPreston Williams\nHunter Henry\n    14\nGolden Tate\nO.J. Howard\nJosh Gordon\nAllen Lazard\nT.J. Hockenson\nDeSean Jackson\nLarry Fitzgerald\nBryan Edwards\nChristopher Herndon\nTEN  Def\n    15\nDevonta Freeman\nLAC  Def\nMIN  Def\nRyquell Armstead\nDK Metcalf\nA.J. Dillon\nSammy Watkins\nIND  Def\nParris Campbell\nMike Gesicki\n  \n  \n  \n\n\n\n\nThis was the initial draft board. My team was the 9th slot. I traded away Julio Jones and cut Odell Beckham Jr. after he went to Injured Reserve. There needs to be a logic to represent these moves, as they are no longer eligible to keep.\nIn addition to structuring the rules, I would like to:\n\nProvide color the names by each player’s position, as is typical on most fantasy football draft boards.\nAllow multiple players to occupy a draft slot. In the case of a traded player, there can be overlap on eligible keepers for one team in a particular round .\n\nThese can not be done with a pivot table (at least not without hardcoding elements). The final output will need to be in a tidy format to allow usage of the grammar of graphics in {ggplot2}.\n\nIneligible Keepers\nTo start with, create a list of the players kept from 2019 in 2020. The players are no longer eligible and need to be filtered out.\nThe NFL and fantasy football is played weekly. Using the {lubridate}, create week 1 of the NFL season from the timestamp field in the transactions data frame.\nTo get the players that were dropped early in advance of their bye week:\n\nFilter for transactions that successfully dropped players\nAdd in each teams bye week\nGet the earliest drop (each player can be added/dropped multiple times throughout the season)\n\nSubset as a list of unique names\n\n\nkept_players <- c(\n  'Lamar Jackson',\n  'Josh Jacobs',\n  'Austin Ekeler',\n  'Aaron Jones',\n  'DK Metcalf',\n  'Kenyan Drake',\n  'Courtland Sutton',\n  'Josh Allen',\n  'D.J. Moore',\n  'Derrick Henry'\n)  \n\ntransactions <- transactions %>% \n  mutate(week = lubridate::week(timestamp) - 36)\n\n\ndropped_players <- transactions %>% \n  filter(type_desc == 'dropped') %>% \n  filter(type != 'waiver_failed') %>% \n  mutate(\n    bye_weeks = case_when(\n      team %in% c(\"PIT\", \"TEN\") ~ 4,\n      team %in% c(\"DEN\", \"DET\", \"GB\", \"NE\") ~ 5,\n      team %in% c(\"LV\", \"LAC\", \"NO\", \"SEA\") ~ 6,\n      team %in% c(\"BAL\", \"IND\", \"MIA\", \"MIN\") ~ 7,\n      team %in% c(\"ARI\", \"HOU\", \"JAX\", \"WAS\") ~ 8,\n      team %in% c(\"CIN\", \"CLE\", \"LAR\", \"PHI\") ~ 9,\n      team %in% c(\"ATL\", \"DAL\", \"KC\", \"NYJ\") ~ 10,\n      team %in% c(\"BUF\", \"CHI\", \"NYG\", \"SF\") ~ 11,\n      team %in% c(\"CAR\", \"TB\") ~ 13\n    )\n  ) %>% \n  group_by(player_name, player_id) %>% \n  arrange(timestamp, player_id) %>% \n  mutate(rn = row_number()) %>% \n  filter(rn == 1) %>% \n  filter(week < bye_weeks - 1) %>% \n  #filter(franchise_id == 1) %>% print(n= 32)\n  pull(player_name) %>% \n  unique()\n\n\n\nTraded Players\n{ffscrapr} does so much of the leg work for you. In the transactions of the trade, a field trade_partner is already included which contains the ID of who the trade went to.\nAgain, follow a similar logic to most recent occurrence of the traded player. For example, Michael Gallup was traded 3 times in our league. He needs to be placed on the final team he ended up on.\nA separate table franchises was created to join the franchise_name to the output.\nFinally, I created a list of the names in our league. If you didn’t see my previous work, you might notice the 9th spot did not turn out so well.\n\nfranchises <- draft_picks %>% \n  count(franchise_id, franchise_name, pick) %>% \n  select(-n) \n\ntrades <- transactions %>% \n  filter(type == 'trade') %>% \n  group_by(player_name) %>% \n  arrange(timestamp) %>% \n  mutate(rn = row_number()) %>% \n  filter(rn == max(rn)) %>% \n  select(franchise_id, franchise_name, player_name, trade_partner) %>%\n  mutate(trade_partner = as.numeric(trade_partner)) %>% \n  left_join(\n    franchises, by = c(\"trade_partner\"=\"franchise_id\"), suffix = c(\"\",\"_trade\")\n  ) \n\nusers <- c(\n  'CHAMP',\n  'Wayne',\n  'Jacob',\n  'Tony',\n  'Ben',\n  'Clayton',\n  'Zach',\n  'Mitch',\n  'Last Place',\n  'Connor'\n)"
  },
  {
    "objectID": "posts/2021-03-21-complicated-keeper-data-manipulation/complicated-keeper-data-manipulation.html#the-plot",
    "href": "posts/2021-03-21-complicated-keeper-data-manipulation/complicated-keeper-data-manipulation.html#the-plot",
    "title": "Complicated Keeper Data Manipulation",
    "section": "The Plot",
    "text": "The Plot\n\ndraft_picks %>% \n  filter(round != 1) %>% \n  mutate(round = round - 1) %>% \n  mutate(url = glue::glue(\n    \"https://sleepercdn.com/content/nfl/players/thumb/{player_id}.jpg\"\n  )) %>% \n  bind_rows(\n    tibble(\n      round = rep(0, 10), \n      player_name = users,\n      pick = 1:10\n      )\n    ) %>% \n  filter(!player_name %in% dropped_players & !player_name %in% kept_players) %>%\n  left_join(\n    trades, \n    by = c(\"franchise_name\", \"player_name\"),\n    suffix = c(\"\", \"_trade\")\n  ) %>% \n  mutate(pick = ifelse(\n    !is.na(pick_trade) & round != 0,\n    pick_trade, \n    pick\n    )) %>% \n  group_by(round, pick) %>% \n  mutate(hjust = n(),\n         hjust_n = row_number()) %>% \n  ungroup() %>% \n  mutate(player_name = case_when(\n    player_name == 'Clyde Edwards-Helaire'~'C. Edwards-Helaire',\n    TRUE ~ player_name\n  )) %>% \n  ggplot(aes(pick, round)) +\n  geom_point(alpha = 0) +\n  geom_label(\n    aes(label = player_name, fill = pos), \n    show.legend = F, \n    data = . %>% filter(round > 0, hjust > 1, hjust_n == 1),\n    vjust = 1,\n    size = 5\n  ) +\n  geom_label(\n    aes(label = player_name, fill = pos),\n    size = 5, \n    show.legend = F, \n    data = . %>% filter(round > 0, hjust > 1, hjust_n == 2),\n    vjust = -0.1\n  ) +\n  geom_label(\n    aes(label = player_name, fill = pos), \n    size = 5, \n    show.legend = F, \n    data = . %>% filter(round > 0, hjust == 1)\n    ) +\n  geom_text(\n    aes(label = player_name),\n    data = . %>% \n      filter(round == 0),\n    size = 7,\n    color = \"#FFFFFF\"\n  ) +\n  scale_y_reverse(breaks = c(1:15)) +\n  theme_minimal() +\n  labs(\n    title = \"Keepers 2021\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_text(color=\"#FFFFFF\", size = 18),\n    plot.title = element_text(size = 30, face = \"bold\", hjust = 0.5, color = \"#FFFFFF\"),\n    plot.background = element_rect(fill = \"#494f5c\"),\n  ) +\n  geom_hline(yintercept = 0.5) +\n  geom_hline(yintercept = seq(1.5, 13.5, 1), alpha = 0.5) +\n  scale_fill_manual(\n    values =  c(\"#d65858\",\"#00ba5d\",\"#ff7c43\", \"#58ffff\")\n  )\n\n\n\n\n\n\nI’ve included the code on how I created the plot, however I’ve cut a corner by not having the code evaluated at runtime, and the static image is passed through.\nI still have a lot to learn with {ggplot2}, especially when it comes to rendering the graphic in dimensions needed. Overall it was a neat problem thinking about how to represent the draftboard.\nThat wraps up this project. Thanks again to Tan for the awesome package!"
  },
  {
    "objectID": "posts/2021-03-21-complicated-keeper-data-manipulation/index.html",
    "href": "posts/2021-03-21-complicated-keeper-data-manipulation/index.html",
    "title": "Fantasy Football Data Wrangling for Keepers",
    "section": "",
    "text": "Working with {ffscrapr}\nMy previous post dove in on sample analysis of our fantasy football league utilizing the API from Sleeper. Since then, I have discovered an amazing package {ffscrapr} on CRAN, developed by Tan. This package does everything for you to get data from Sleeper into easy to work with data frames."
  },
  {
    "objectID": "posts/2021-03-21-complicated-keeper-data-manipulation/index.html#the-goal",
    "href": "posts/2021-03-21-complicated-keeper-data-manipulation/index.html#the-goal",
    "title": "Fantasy Football Data Wrangling for Keepers",
    "section": "The Goal",
    "text": "The Goal\n\n\n\n\n\nEach year we draft players for the upcoming season, which is typically referred to as redraft format. A couple years ago we began a keeper format where we can keep a player from the previous year.\nThere are several ways to implement a keeper format, our rules are as follows:\n\nKeep 1 player, that player can not be kept consecutive years\n\nThe kept player will be kept at the round they were drafted in with a single round penalty (i.e. if you drafted a player in round 10, you would keep them in round 9 the following year)\n\nIf you trade a player, the new team gets the rights to keep that player\n\nThe player must remain on the roster at least until the week before their respective bye week\n\nAs you can imagine, this is a nightmare to track in a spreadsheet as a commissioner."
  },
  {
    "objectID": "posts/2021-03-21-complicated-keeper-data-manipulation/index.html#the-solution",
    "href": "posts/2021-03-21-complicated-keeper-data-manipulation/index.html#the-solution",
    "title": "Fantasy Football Data Wrangling for Keepers",
    "section": "The Solution",
    "text": "The Solution\nOnly a few packages needed (if you count {tidyverse} as a few that is) to begin the analysis.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
  },
  {
    "objectID": "posts/2021-03-21-complicated-keeper-data-manipulation/index.html#ffscrapr",
    "href": "posts/2021-03-21-complicated-keeper-data-manipulation/index.html#ffscrapr",
    "title": "Fantasy Football Data Wrangling for Keepers",
    "section": "ffscrapr",
    "text": "ffscrapr\nTan has well documented how the package works through the {ffscraper} website. I won’t go into too much detail, as the API is fairly simple to understand from the function names alone. Basically, create a league object and get the draft picks and transactions for it.\nThe league I will be focusing on is a leauge is called The Hot Boyz… Hopefully Dallas Cowboys fans understand!\n\nmy_leagues <- ffscrapr::sleeper_userleagues(\"rplain\", 2020)\n\nleague_id <- my_leagues %>% \n  filter(league_name == 'The Hot Boyz') %>% \n  pull(league_id)\n\nmy_league <- ffscrapr::ff_connect(platform = 'sleeper', \n                                  season = 2020, \n                                  league_id = league_id)\n\ntransactions <- ff_transactions(my_league)\n\ndraft_picks <- ff_draft(my_league) \n\n\nBelow is what the draft board looked like following the draft.\n\ndraft_picks %>% \n  select(round, pick, franchise_name, player_name, pos, team) %>% \n   mutate(player_name = ifelse(is.na(player_name), paste(team, \" Def\"), player_name)) %>% \n  pivot_wider(\n    id_cols = round,\n    names_from = franchise_name,\n    values_from = player_name\n  ) %>% \n  `colnames<-`(c(\"Round\",1:10)) %>% \n  gt::gt()\n\n\n\n\n\n  \n  \n    \n      Round\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n    \n  \n  \n    1\nChristian McCaffrey\nEzekiel Elliott\nSaquon Barkley\nMichael Thomas\nDalvin Cook\nAlvin Kamara\nLe'Veon Bell\nJoe Mixon\nMiles Sanders\nDavante Adams\n    2\nChris Godwin\nJosh Jacobs\nDeAndre Hopkins\nTravis Kelce\nTyreek Hill\nPatrick Mahomes\nNick Chubb\nChris Carson\nJulio Jones\nClyde Edwards-Helaire\n    3\nJames Conner\nMike Evans\nAustin Ekeler\nDavid Johnson\nGeorge Kittle\nAdam Thielen\nTodd Gurley\nKenny Golladay\nJonathan Taylor\nMark Andrews\n    4\nRobert Woods\nJuJu Smith-Schuster\nAmari Cooper\nAaron Jones\nMelvin Gordon\nCalvin Ridley\nRaheem Mostert\nAllen Robinson\nOdell Beckham\nDerrick Henry\n    5\nMark Ingram\nA.J. Brown\nKareem Hunt\nCooper Kupp\nD.J. Chark\nZach Ertz\nStefon Diggs\nKeenan Allen\nD.J. Moore\nTerry McLaurin\n    6\nMichael Gallup\nMarquise Brown\nDevin Singletary\nRussell Wilson\nDak Prescott\nKenyan Drake\nTyler Lockett\nT.Y. Hilton\nCam Akers\nDeshaun Watson\n    7\nAntonio Gibson\nJ.K. Dobbins\nDarren Waller\nDavid Montgomery\nLeonard Fournette\nDeVante Parker\nWill Fuller\nMarlon Mack\nZack Moss\nTyler Boyd\n    8\nBrandin Cooks\nRonald Jones\nA.J. Green\nScott Miller\nSF  Def\nJames White\nRob Gronkowski\nEvan Engram\nKyler Murray\nJulian Edelman\n    9\nLamar Jackson\nCeeDee Lamb\nJerry Jeudy\nJordan Howard\nDiontae Johnson\nHenry Ruggs\nTyler Higbee\nD'Andre Swift\nDeebo Samuel\nJarvis Landry\n    10\nNoah Fant\nJared Cook\nMatt Ryan\nHayden Hurst\nEmmanuel Sanders\nAustin Hooper\nCam Newton\nMarvin Jones\nTarik Cohen\nMatt Breida\n    11\nPIT  Def\nTom Brady\nJohn Brown\nPhillip Lindsay\nChristian Kirk\nBAL  Def\nNE  Def\nChris Thompson\nChase Edmonds\nJamison Crowder\n    12\nTony Pollard\nKerryon Johnson\nJustin Jefferson\nBUF  Def\nTevin Coleman\nSony Michel\nSterling Shepard\nLatavius Murray\nDamien Harris\nAaron Rodgers\n    13\nAnthony Miller\nMecole Hardman\nBlake Jarwin\nAlexander Mattison\nDarius Slayton\nJoe Burrow\nCourtland Sutton\nJosh Allen\nPreston Williams\nHunter Henry\n    14\nGolden Tate\nO.J. Howard\nJosh Gordon\nAllen Lazard\nT.J. Hockenson\nDeSean Jackson\nLarry Fitzgerald\nBryan Edwards\nChristopher Herndon\nTEN  Def\n    15\nDevonta Freeman\nLAC  Def\nMIN  Def\nRyquell Armstead\nDK Metcalf\nA.J. Dillon\nSammy Watkins\nIND  Def\nParris Campbell\nMike Gesicki\n  \n  \n  \n\n\n\n\nThis was the initial draft board. My team was the 9th slot. I traded away Julio Jones and cut Odell Beckham Jr. after he went to Injured Reserve. There needs to be a logic to represent these moves, as they are no longer eligible to keep.\nIn addition to structuring the rules, I would like to:\n\nProvide color the names by each player’s position, as is typical on most fantasy football draft boards.\nAllow multiple players to occupy a draft slot. In the case of a traded player, there can be overlap on eligible keepers for one team in a particular round .\n\nThese can not be done with a pivot table (at least not without hardcoding elements). The final output will need to be in a tidy format to allow usage of the grammar of graphics in {ggplot2}.\n\nIneligible Keepers\nTo start with, create a list of the players kept from 2019 in 2020. The players are no longer eligible and need to be filtered out.\nThe NFL and fantasy football is played weekly. Using the {lubridate}, create week 1 of the NFL season from the timestamp field in the transactions data frame.\nTo get the players that were dropped early in advance of their bye week:\n\nFilter for transactions that successfully dropped players\nAdd in each teams bye week\nGet the earliest drop (each player can be added/dropped multiple times throughout the season)\n\nSubset as a list of unique names\n\n\nkept_players <- c(\n  'Lamar Jackson',\n  'Josh Jacobs',\n  'Austin Ekeler',\n  'Aaron Jones',\n  'DK Metcalf',\n  'Kenyan Drake',\n  'Courtland Sutton',\n  'Josh Allen',\n  'D.J. Moore',\n  'Derrick Henry'\n)  \n\ntransactions <- transactions %>% \n  mutate(week = lubridate::week(timestamp) - 36)\n\n\ndropped_players <- transactions %>% \n  filter(type_desc == 'dropped') %>% \n  filter(type != 'waiver_failed') %>% \n  mutate(\n    bye_weeks = case_when(\n      team %in% c(\"PIT\", \"TEN\") ~ 4,\n      team %in% c(\"DEN\", \"DET\", \"GB\", \"NE\") ~ 5,\n      team %in% c(\"LV\", \"LAC\", \"NO\", \"SEA\") ~ 6,\n      team %in% c(\"BAL\", \"IND\", \"MIA\", \"MIN\") ~ 7,\n      team %in% c(\"ARI\", \"HOU\", \"JAX\", \"WAS\") ~ 8,\n      team %in% c(\"CIN\", \"CLE\", \"LAR\", \"PHI\") ~ 9,\n      team %in% c(\"ATL\", \"DAL\", \"KC\", \"NYJ\") ~ 10,\n      team %in% c(\"BUF\", \"CHI\", \"NYG\", \"SF\") ~ 11,\n      team %in% c(\"CAR\", \"TB\") ~ 13\n    )\n  ) %>% \n  group_by(player_name, player_id) %>% \n  arrange(timestamp, player_id) %>% \n  mutate(rn = row_number()) %>% \n  filter(rn == 1) %>% \n  filter(week < bye_weeks - 1) %>% \n  #filter(franchise_id == 1) %>% print(n= 32)\n  pull(player_name) %>% \n  unique()\n\n\n\nTraded Players\n{ffscrapr} does so much of the leg work for you. In the transactions of the trade, a field trade_partner is already included which contains the ID of who the trade went to.\nAgain, follow a similar logic to most recent occurrence of the traded player. For example, Michael Gallup was traded 3 times in our league. He needs to be placed on the final team he ended up on.\nA separate table franchises was created to join the franchise_name to the output.\nFinally, I created a list of the names in our league. If you didn’t see my previous work, you might notice the 9th spot did not turn out so well.\n\nfranchises <- draft_picks %>% \n  count(franchise_id, franchise_name, pick) %>% \n  select(-n) \n\ntrades <- transactions %>% \n  filter(type == 'trade') %>% \n  group_by(player_name) %>% \n  arrange(timestamp) %>% \n  mutate(rn = row_number()) %>% \n  filter(rn == max(rn)) %>% \n  select(franchise_id, franchise_name, player_name, trade_partner) %>%\n  mutate(trade_partner = as.numeric(trade_partner)) %>% \n  left_join(\n    franchises, by = c(\"trade_partner\"=\"franchise_id\"), suffix = c(\"\",\"_trade\")\n  ) \n\nusers <- c(\n  'CHAMP',\n  'Wayne',\n  'Jacob',\n  'Tony',\n  'Ben',\n  'Clayton',\n  'Zach',\n  'Mitch',\n  'Last Place',\n  'Connor'\n)"
  },
  {
    "objectID": "posts/2021-03-21-complicated-keeper-data-manipulation/index.html#the-plot",
    "href": "posts/2021-03-21-complicated-keeper-data-manipulation/index.html#the-plot",
    "title": "Fantasy Football Data Wrangling for Keepers",
    "section": "The Plot",
    "text": "The Plot\n\ndraft_picks %>% \n  filter(round != 1) %>% \n  mutate(round = round - 1) %>% \n  mutate(url = glue::glue(\n    \"https://sleepercdn.com/content/nfl/players/thumb/{player_id}.jpg\"\n  )) %>% \n  bind_rows(\n    tibble(\n      round = rep(0, 10), \n      player_name = users,\n      pick = 1:10\n      )\n    ) %>% \n  filter(!player_name %in% dropped_players & !player_name %in% kept_players) %>%\n  left_join(\n    trades, \n    by = c(\"franchise_name\", \"player_name\"),\n    suffix = c(\"\", \"_trade\")\n  ) %>% \n  mutate(pick = ifelse(\n    !is.na(pick_trade) & round != 0,\n    pick_trade, \n    pick\n    )) %>% \n  group_by(round, pick) %>% \n  mutate(hjust = n(),\n         hjust_n = row_number()) %>% \n  ungroup() %>% \n  mutate(player_name = case_when(\n    player_name == 'Clyde Edwards-Helaire'~'C. Edwards-Helaire',\n    TRUE ~ player_name\n  )) %>% \n  ggplot(aes(pick, round)) +\n  geom_point(alpha = 0) +\n  geom_label(\n    aes(label = player_name, fill = pos), \n    show.legend = F, \n    data = . %>% filter(round > 0, hjust > 1, hjust_n == 1),\n    vjust = 1,\n    size = 5\n  ) +\n  geom_label(\n    aes(label = player_name, fill = pos),\n    size = 5, \n    show.legend = F, \n    data = . %>% filter(round > 0, hjust > 1, hjust_n == 2),\n    vjust = -0.1\n  ) +\n  geom_label(\n    aes(label = player_name, fill = pos), \n    size = 5, \n    show.legend = F, \n    data = . %>% filter(round > 0, hjust == 1)\n    ) +\n  geom_text(\n    aes(label = player_name),\n    data = . %>% \n      filter(round == 0),\n    size = 7,\n    color = \"#FFFFFF\"\n  ) +\n  scale_y_reverse(breaks = c(1:15)) +\n  theme_minimal() +\n  labs(\n    title = \"Keepers 2021\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_text(color=\"#FFFFFF\", size = 18),\n    plot.title = element_text(size = 30, face = \"bold\", hjust = 0.5, color = \"#FFFFFF\"),\n    plot.background = element_rect(fill = \"#494f5c\"),\n  ) +\n  geom_hline(yintercept = 0.5) +\n  geom_hline(yintercept = seq(1.5, 13.5, 1), alpha = 0.5) +\n  scale_fill_manual(\n    values =  c(\"#d65858\",\"#00ba5d\",\"#ff7c43\", \"#58ffff\")\n  )\n\n\n\n\n\n\nI’ve included the code on how I created the plot, however I’ve cut a corner by not having the code evaluated at runtime, and the static image is passed through.\nI still have a lot to learn with {ggplot2}, especially when it comes to rendering the graphic in dimensions needed. Overall it was a neat problem thinking about how to represent the draftboard.\nThat wraps up this project. Thanks again to Tan for the awesome package!"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nClover | Data Analyst\nSouthwest Airlines | Data Analyst"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nGeorgia Institute of Technology | Atlanta, GA\nM.S. in Analytics\nThe University of North Texas | Denton, TX\nB.B.A. in Business Analytics"
  },
  {
    "objectID": "posts/in-progress/index.html",
    "href": "posts/in-progress/index.html",
    "title": "Untitled",
    "section": "",
    "text": "What is the reasoning behind this post? I love what I do, and can get an abundance of energy from projects that I’m working on. That being said, it’s incredibly frustrating using a specific tool, when I would enjoy and work faster in another tool.\n\n\nNothing will earn an auto-mute from me on Twitter faster than seeing a Python vs R debate. They are both fantastic tools, and each have their own strengths and weaknesses. What I find most important is to focus on how they can present themselves to a particular user. A feature I see as beneficial from R could be detrimental to another user, for a myriad of reasons.\nI support that setting up infrastructure which enables data professionals to use their tool of choice will help flourish the flow of ideas and analysis. Especially early on in the data exploration phase."
  },
  {
    "objectID": "posts/in-progress/index.html#setup",
    "href": "posts/in-progress/index.html#setup",
    "title": "Untitled",
    "section": "Setup",
    "text": "Setup\n\nDuckDB\nDuckDB is awesome. Check it out.\nAgain, this is outside the scope of this post. There is already an extensive amount of coverage on how the DB works, what it’s best for, and examples of how to use it. I mainly chose this because of it’s integration into the Apache Arrow ecosystem.\nThere are many different interfaces to install DuckDB found here: DuckDB Install.\n\n\nReticulate Config\nUsing Reticulate, we can integrate Python and R. At work, I created an R package that would allow me to use our established pipelines in python and analyze the data in R with Reticulate.\nThe intended user is already someone that is familiar with R (after all what is the point of all of this if you don’t already have {dplyr} installed?!).\n\nif(!require('reticulate')) install.packages('reticulate') \nif(!require('arrow')) install.packages('arrow') \nif(!require('tictoc')) install.packages('tictoc') # only needed for benchmark\n\nif (!reticulate::virtualenv_exists(\"demo_env\")) {\n  reticulate::virtualenv_create(\n    'demo_env'\n    , packages = c('duckdb'\n                   , 'pandas==2.0'\n                   , 'polars'\n                   , 'pyarrow'\n                   , 'scikit-learn'\n                   )\n    )\n}\n\nreticulate::use_virtualenv('demo_env')"
  },
  {
    "objectID": "posts/in-progress/index.html#python-db-connect",
    "href": "posts/in-progress/index.html#python-db-connect",
    "title": "Untitled",
    "section": "Python DB Connect",
    "text": "Python DB Connect\n\nimport duckdb \nimport pandas as pd\nimport polars as pl\nimport pyarrow as pa\nfrom sklearn import datasets\n\ndata = datasets.load_wine(as_frame=True)['data']\n\n# Create a DuckDB connection\n#conn = duckdb.connect(\"posts/in-progress/data/demo.db\")\nconn = duckdb.connect(\"data/demo.db\")\n\n# Create toy data\nduckdb.sql(\"\"\"\nDROP TABLE IF EXISTS my_table;\nCREATE TABLE my_table AS SELECT * FROM data;\nINSERT INTO my_table SELECT * FROM data;\n\"\"\")\n\n# Check that table is created\nduckdb.sql(\"SELECT * FROM my_table LIMIT 10;\")\n\n┌─────────┬────────────┬────────┬───────────────────┬───┬─────────────────┬────────┬──────────────────────┬─────────┐\n│ alcohol │ malic_acid │  ash   │ alcalinity_of_ash │ … │ color_intensity │  hue   │ od280/od315_of_dil…  │ proline │\n│ double  │   double   │ double │      double       │   │     double      │ double │        double        │ double  │\n├─────────┼────────────┼────────┼───────────────────┼───┼─────────────────┼────────┼──────────────────────┼─────────┤\n│   14.23 │       1.71 │   2.43 │              15.6 │ … │            5.64 │   1.04 │                 3.92 │  1065.0 │\n│    13.2 │       1.78 │   2.14 │              11.2 │ … │            4.38 │   1.05 │                  3.4 │  1050.0 │\n│   13.16 │       2.36 │   2.67 │              18.6 │ … │            5.68 │   1.03 │                 3.17 │  1185.0 │\n│   14.37 │       1.95 │    2.5 │              16.8 │ … │             7.8 │   0.86 │                 3.45 │  1480.0 │\n│   13.24 │       2.59 │   2.87 │              21.0 │ … │            4.32 │   1.04 │                 2.93 │   735.0 │\n│    14.2 │       1.76 │   2.45 │              15.2 │ … │            6.75 │   1.05 │                 2.85 │  1450.0 │\n│   14.39 │       1.87 │   2.45 │              14.6 │ … │            5.25 │   1.02 │                 3.58 │  1290.0 │\n│   14.06 │       2.15 │   2.61 │              17.6 │ … │            5.05 │   1.06 │                 3.58 │  1295.0 │\n│   14.83 │       1.64 │   2.17 │              14.0 │ … │             5.2 │   1.08 │                 2.85 │  1045.0 │\n│   13.86 │       1.35 │   2.27 │              16.0 │ … │            7.22 │   1.01 │                 3.55 │  1045.0 │\n├─────────┴────────────┴────────┴───────────────────┴───┴─────────────────┴────────┴──────────────────────┴─────────┤\n│ 10 rows                                                                                      13 columns (8 shown) │\n└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
  },
  {
    "objectID": "posts/in-progress/index.html#pyarrow",
    "href": "posts/in-progress/index.html#pyarrow",
    "title": "Untitled",
    "section": "PyArrow",
    "text": "PyArrow\nNow that we have a database connection, we can run SQL or use existing Python code to retrieve our data.\nDuckDB and my workflow can return and Arrow object. If your use case can’t and returns a pandas dataframe, you will need PyArrow to convert it.\n\n\ndf_pyarrow = duckdb.sql('SELECT * FROM my_table').fetch_arrow_table()\n\n# If what you are using returns pandas dataframe\ndf_pyarrow_pandas = pa.Table.from_pandas(data)"
  },
  {
    "objectID": "posts/in-progress/index.html#seemless-conversion-to-r",
    "href": "posts/in-progress/index.html#seemless-conversion-to-r",
    "title": "Untitled",
    "section": "Seemless conversion to R",
    "text": "Seemless conversion to R\nNow the wine dataset is simple enough to work with in Python. With this size, writing a csv or parquet file is even feasible.\nHowever, if you have data that is 10+ million rows, that isn’t going to be a sustainable solution. How do you transfer the data while reducing I/O constraints as much as possible?\n\nArrow\nI’ve mentioned Arrow many times throughout this post, and will continue to reference other sources for further understanding. A high level overview is that it is a standardized memory format for data, independent of language or tooling.\nIn the most basic use case of transferring a Pandas dataframe to R, there is a conversion of how it was stored in memory for Pandas and a mapping of how it will be stored in memory for an R data.frame(). To do that requires copying the data. This is called Serialization.\nWith Arrow, that definition is constant and allows for zero-copy reads without serialization.\n\n\nFinal Product\nNow this part is absolutely silly, the only piece that was missing from Danielle’s article was that py_to_r() wasn’t even needed. All I had to do was assign an r variable with the Python object with Reticulate: df <- py$df.\n\ntictoc::tic()\ndf_arrrow1 <- reticulate::py$df_pyarrow\ntictoc::toc()\n\n0.014 sec elapsed\n\ndf_arrrow1 |> \n  utils::head() |> \n  dplyr::collect()\n\n# A tibble: 6 × 13\n  alcohol malic_acid   ash alcalinity_of_ash magnesium total_phenols flavanoids\n    <dbl>      <dbl> <dbl>             <dbl>     <dbl>         <dbl>      <dbl>\n1    14.2       1.71  2.43              15.6       127          2.8        3.06\n2    13.2       1.78  2.14              11.2       100          2.65       2.76\n3    13.2       2.36  2.67              18.6       101          2.8        3.24\n4    14.4       1.95  2.5               16.8       113          3.85       3.49\n5    13.2       2.59  2.87              21         118          2.8        2.69\n6    14.2       1.76  2.45              15.2       112          3.27       3.39\n# ℹ 6 more variables: nonflavanoid_phenols <dbl>, proanthocyanins <dbl>,\n#   color_intensity <dbl>, hue <dbl>, `od280/od315_of_diluted_wines` <dbl>,\n#   proline <dbl>\n\n# If pandas to arrow needed\ndf_arrow2 <- reticulate::py$df_pyarrow_pandas\ndf_arrrow1 |> \n  utils::head() |> \n  dplyr::collect()\n\n# A tibble: 6 × 13\n  alcohol malic_acid   ash alcalinity_of_ash magnesium total_phenols flavanoids\n    <dbl>      <dbl> <dbl>             <dbl>     <dbl>         <dbl>      <dbl>\n1    14.2       1.71  2.43              15.6       127          2.8        3.06\n2    13.2       1.78  2.14              11.2       100          2.65       2.76\n3    13.2       2.36  2.67              18.6       101          2.8        3.24\n4    14.4       1.95  2.5               16.8       113          3.85       3.49\n5    13.2       2.59  2.87              21         118          2.8        2.69\n6    14.2       1.76  2.45              15.2       112          3.27       3.39\n# ℹ 6 more variables: nonflavanoid_phenols <dbl>, proanthocyanins <dbl>,\n#   color_intensity <dbl>, hue <dbl>, `od280/od315_of_diluted_wines` <dbl>,\n#   proline <dbl>\n\n\nI’m off to go work in my preferred environment…"
  },
  {
    "objectID": "posts/in-progress/index.html#use-case",
    "href": "posts/in-progress/index.html#use-case",
    "title": "Untitled",
    "section": "Use Case",
    "text": "Use Case\nFor reasons I won’t get into here, at work I needed to connect to a database in Python using existing code. From there I could stay in that environment, but I wanted to be able to work with the data in a tool that is better suited for me in R.\nI was familiar with the Apache Arrow project, and the best source I found for understanding from an R user’s perspective was This Blog Post. Danielle, at the time of writing, is a Developer Advocate at Voltron Data. A couple of statements from their website “Bridging Languages, Hardware, and People” along with “Accelerate Success with the Apache Arrow Ecosystem”.\n\nMinor Improvements\nDanielle covers all the foundations of reticulate, Apache Arrow, and getting everything set up. I was able to apply it for my use case, with one drawback. The post mentions r_to_py(), and when I tried to use py_to_r(), I had no success."
  },
  {
    "objectID": "posts/in-progress/index.html#section",
    "href": "posts/in-progress/index.html#section",
    "title": "Untitled",
    "section": "",
    "text": "As mentioned above, DuckDB is used for the toy use case. For my workflow in particular, this is where I would connect to our data with the existing Python codebase. The idea is to not re-invent the wheel for something that is already working. Rather, build a tire for that wheel that will make you go faster.\nBelow creates a toy dataset from Scikit-Learn datasets, Wine.\nNote: You can make this virtual environment lighter by not having to install sklearn and duckdb.\n\nimport duckdb \nimport pandas as pd\nimport polars as pl\nimport pyarrow as pa\nfrom sklearn import datasets\n\ndata = datasets.load_wine(as_frame=True)['data']\n\n# Create a DuckDB connection\n#conn = duckdb.connect(\"posts/in-progress/data/demo.db\")\nconn = duckdb.connect(\"data/demo.db\")\n\n# Create toy data\nduckdb.sql(\"\"\"\nDROP TABLE IF EXISTS my_table;\nCREATE TABLE my_table AS SELECT * FROM data;\nINSERT INTO my_table SELECT * FROM data;\n\"\"\")\n\n# Check that table is created\nduckdb.sql(\"SELECT * FROM my_table LIMIT 10;\")\n\n┌─────────┬────────────┬────────┬───────────────────┬───┬─────────────────┬────────┬──────────────────────┬─────────┐\n│ alcohol │ malic_acid │  ash   │ alcalinity_of_ash │ … │ color_intensity │  hue   │ od280/od315_of_dil…  │ proline │\n│ double  │   double   │ double │      double       │   │     double      │ double │        double        │ double  │\n├─────────┼────────────┼────────┼───────────────────┼───┼─────────────────┼────────┼──────────────────────┼─────────┤\n│   14.23 │       1.71 │   2.43 │              15.6 │ … │            5.64 │   1.04 │                 3.92 │  1065.0 │\n│    13.2 │       1.78 │   2.14 │              11.2 │ … │            4.38 │   1.05 │                  3.4 │  1050.0 │\n│   13.16 │       2.36 │   2.67 │              18.6 │ … │            5.68 │   1.03 │                 3.17 │  1185.0 │\n│   14.37 │       1.95 │    2.5 │              16.8 │ … │             7.8 │   0.86 │                 3.45 │  1480.0 │\n│   13.24 │       2.59 │   2.87 │              21.0 │ … │            4.32 │   1.04 │                 2.93 │   735.0 │\n│    14.2 │       1.76 │   2.45 │              15.2 │ … │            6.75 │   1.05 │                 2.85 │  1450.0 │\n│   14.39 │       1.87 │   2.45 │              14.6 │ … │            5.25 │   1.02 │                 3.58 │  1290.0 │\n│   14.06 │       2.15 │   2.61 │              17.6 │ … │            5.05 │   1.06 │                 3.58 │  1295.0 │\n│   14.83 │       1.64 │   2.17 │              14.0 │ … │             5.2 │   1.08 │                 2.85 │  1045.0 │\n│   13.86 │       1.35 │   2.27 │              16.0 │ … │            7.22 │   1.01 │                 3.55 │  1045.0 │\n├─────────┴────────────┴────────┴───────────────────┴───┴─────────────────┴────────┴──────────────────────┴─────────┤\n│ 10 rows                                                                                      13 columns (8 shown) │\n└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
  },
  {
    "objectID": "posts/2023-06-19-py-to-r/index.html",
    "href": "posts/2023-06-19-py-to-r/index.html",
    "title": "Arrow, Python, & R",
    "section": "",
    "text": "Supporting R in a Python world, using Arrow."
  },
  {
    "objectID": "posts/2023-06-19-py-to-r/index.html#use-case",
    "href": "posts/2023-06-19-py-to-r/index.html#use-case",
    "title": "Arrow, Python, & R",
    "section": "Use Case",
    "text": "Use Case\nFor reasons I won’t get into here, at work I needed to connect to a database in Python using existing code. From there I could stay in that environment, but I wanted to be able to work with the data in a tool that is better suited for me in R.\nI was familiar with the Apache Arrow project, and the best source I found for understanding from an R user’s perspective was This Blog Post. Danielle, at the time of writing, is a Developer Advocate at Voltron Data. A couple of statements from their website “Bridging Languages, Hardware, and People” along with “Accelerate Success with the Apache Arrow Ecosystem”.\n\nMinor Improvements\nDanielle covers all the foundations of reticulate, Apache Arrow, and getting everything set up. I was able to apply it for my use case, with one drawback. The post mentions r_to_py(), and when I tried to use py_to_r(), I had no success."
  },
  {
    "objectID": "posts/2023-06-19-py-to-r/index.html#setup",
    "href": "posts/2023-06-19-py-to-r/index.html#setup",
    "title": "Arrow, Python, & R",
    "section": "Setup",
    "text": "Setup\n\nDuckDB\nDuckDB is awesome. Check it out.\nAgain, this is outside the scope of this post. There is already an extensive amount of coverage on how the DB works, what it’s best for, and examples of how to use it. I mainly chose this because of it’s integration into the Apache Arrow ecosystem.\nThere are many different interfaces to install DuckDB found here: DuckDB Install.\n\n\nReticulate Config\nUsing Reticulate, we can integrate Python and R. At work, I created an R package that would allow me to use our established pipelines in python and analyze the data in R with Reticulate.\nThe intended user is already someone that is familiar with R (after all what is the point of all of this if you don’t already have {dplyr} installed?!).\n\nif(!require('reticulate')) install.packages('reticulate') \nif(!require('arrow')) install.packages('arrow') \nif(!require('tictoc')) install.packages('tictoc') # only needed for benchmark\n\nif (!reticulate::virtualenv_exists(\"demo_env\")) {\n  reticulate::virtualenv_create(\n    'demo_env'\n    , packages = c('duckdb'\n                   , 'pandas==2.0'\n                   , 'polars'\n                   , 'pyarrow'\n                   , 'scikit-learn'\n                   )\n    )\n}\n\nreticulate::use_virtualenv('demo_env')"
  },
  {
    "objectID": "posts/2023-06-19-py-to-r/index.html#section",
    "href": "posts/2023-06-19-py-to-r/index.html#section",
    "title": "Arrow, Python, & R",
    "section": "",
    "text": "As mentioned above, DuckDB is used for the toy use case. For my workflow in particular, this is where I would connect to our data with the existing Python codebase. The idea is to not re-invent the wheel for something that is already working. Rather, build a tire for that wheel that will make you go faster.\nBelow creates a toy dataset from Scikit-Learn datasets, Wine.\nNote: You can make this virtual environment lighter by not having to install sklearn and duckdb.\n\nimport duckdb \nimport pandas as pd\nimport polars as pl\nimport pyarrow as pa\nfrom sklearn import datasets\n\ndata = datasets.load_wine(as_frame=True)['data']\n\n# Create a DuckDB connection\n#conn = duckdb.connect(\"posts/in-progress/data/demo.db\")\nconn = duckdb.connect(\"data/demo.db\")\n\n# Create toy data\nduckdb.sql(\"\"\"\nDROP TABLE IF EXISTS my_table;\nCREATE TABLE my_table AS SELECT * FROM data;\nINSERT INTO my_table SELECT * FROM data;\n\"\"\")\n\n# Check that table is created\nduckdb.sql(\"SELECT * FROM my_table LIMIT 10;\")\n\n┌─────────┬────────────┬────────┬───────────────────┬───┬─────────────────┬────────┬──────────────────────┬─────────┐\n│ alcohol │ malic_acid │  ash   │ alcalinity_of_ash │ … │ color_intensity │  hue   │ od280/od315_of_dil…  │ proline │\n│ double  │   double   │ double │      double       │   │     double      │ double │        double        │ double  │\n├─────────┼────────────┼────────┼───────────────────┼───┼─────────────────┼────────┼──────────────────────┼─────────┤\n│   14.23 │       1.71 │   2.43 │              15.6 │ … │            5.64 │   1.04 │                 3.92 │  1065.0 │\n│    13.2 │       1.78 │   2.14 │              11.2 │ … │            4.38 │   1.05 │                  3.4 │  1050.0 │\n│   13.16 │       2.36 │   2.67 │              18.6 │ … │            5.68 │   1.03 │                 3.17 │  1185.0 │\n│   14.37 │       1.95 │    2.5 │              16.8 │ … │             7.8 │   0.86 │                 3.45 │  1480.0 │\n│   13.24 │       2.59 │   2.87 │              21.0 │ … │            4.32 │   1.04 │                 2.93 │   735.0 │\n│    14.2 │       1.76 │   2.45 │              15.2 │ … │            6.75 │   1.05 │                 2.85 │  1450.0 │\n│   14.39 │       1.87 │   2.45 │              14.6 │ … │            5.25 │   1.02 │                 3.58 │  1290.0 │\n│   14.06 │       2.15 │   2.61 │              17.6 │ … │            5.05 │   1.06 │                 3.58 │  1295.0 │\n│   14.83 │       1.64 │   2.17 │              14.0 │ … │             5.2 │   1.08 │                 2.85 │  1045.0 │\n│   13.86 │       1.35 │   2.27 │              16.0 │ … │            7.22 │   1.01 │                 3.55 │  1045.0 │\n├─────────┴────────────┴────────┴───────────────────┴───┴─────────────────┴────────┴──────────────────────┴─────────┤\n│ 10 rows                                                                                      13 columns (8 shown) │\n└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
  },
  {
    "objectID": "posts/2023-06-19-py-to-r/index.html#pyarrow",
    "href": "posts/2023-06-19-py-to-r/index.html#pyarrow",
    "title": "Arrow, Python, & R",
    "section": "PyArrow",
    "text": "PyArrow\nNow that we have a database connection, we can run SQL or use existing Python code to retrieve our data.\nDuckDB and my workflow can return and Arrow object. If your use case can’t and returns a pandas dataframe, you will need PyArrow to convert it.\n\n\ndf_pyarrow = duckdb.sql('SELECT * FROM my_table').fetch_arrow_table()\n\n# If what you are using returns pandas dataframe\ndf_pyarrow_pandas = pa.Table.from_pandas(data)"
  },
  {
    "objectID": "posts/2023-06-19-py-to-r/index.html#seemless-conversion-to-r",
    "href": "posts/2023-06-19-py-to-r/index.html#seemless-conversion-to-r",
    "title": "Arrow, Python, & R",
    "section": "Seemless conversion to R",
    "text": "Seemless conversion to R\nNow the wine dataset is simple enough to work with in Python. With this size, writing a csv or parquet file is even feasible.\nHowever, if you have data that is 10+ million rows, that isn’t going to be a sustainable solution. How do you transfer the data while reducing I/O constraints as much as possible?\n\nArrow\nI’ve mentioned Arrow many times throughout this post, and will continue to reference other sources for further understanding. A high level overview is that it is a standardized memory format for data, independent of language or tooling.\nIn the most basic use case of transferring a Pandas dataframe to R, there is a conversion of how it was stored in memory for Pandas and a mapping of how it will be stored in memory for an R data.frame(). To do that requires copying the data. This is called Serialization.\nWith Arrow, that definition is constant and allows for zero-copy reads without serialization.\n\n\nFinal Product\nNow this part is absolutely silly, the only piece that was missing from Danielle’s article was that py_to_r() wasn’t even needed. All I had to do was assign an r variable with the Python object with Reticulate: df &lt;- py$df.\n\ntictoc::tic()\ndf_arrrow1 &lt;- reticulate::py$df_pyarrow\ntictoc::toc()\n\n0.014 sec elapsed\n\ndf_arrrow1 |&gt; \n  utils::head() |&gt; \n  dplyr::collect()\n\n# A tibble: 6 × 13\n  alcohol malic_acid   ash alcalinity_of_ash magnesium total_phenols flavanoids\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1    14.2       1.71  2.43              15.6       127          2.8        3.06\n2    13.2       1.78  2.14              11.2       100          2.65       2.76\n3    13.2       2.36  2.67              18.6       101          2.8        3.24\n4    14.4       1.95  2.5               16.8       113          3.85       3.49\n5    13.2       2.59  2.87              21         118          2.8        2.69\n6    14.2       1.76  2.45              15.2       112          3.27       3.39\n# ℹ 6 more variables: nonflavanoid_phenols &lt;dbl&gt;, proanthocyanins &lt;dbl&gt;,\n#   color_intensity &lt;dbl&gt;, hue &lt;dbl&gt;, `od280/od315_of_diluted_wines` &lt;dbl&gt;,\n#   proline &lt;dbl&gt;\n\n# If pandas to arrow needed\ndf_arrow2 &lt;- reticulate::py$df_pyarrow_pandas\ndf_arrrow1 |&gt; \n  utils::head() |&gt; \n  dplyr::collect()\n\n# A tibble: 6 × 13\n  alcohol malic_acid   ash alcalinity_of_ash magnesium total_phenols flavanoids\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1    14.2       1.71  2.43              15.6       127          2.8        3.06\n2    13.2       1.78  2.14              11.2       100          2.65       2.76\n3    13.2       2.36  2.67              18.6       101          2.8        3.24\n4    14.4       1.95  2.5               16.8       113          3.85       3.49\n5    13.2       2.59  2.87              21         118          2.8        2.69\n6    14.2       1.76  2.45              15.2       112          3.27       3.39\n# ℹ 6 more variables: nonflavanoid_phenols &lt;dbl&gt;, proanthocyanins &lt;dbl&gt;,\n#   color_intensity &lt;dbl&gt;, hue &lt;dbl&gt;, `od280/od315_of_diluted_wines` &lt;dbl&gt;,\n#   proline &lt;dbl&gt;\n\n\n\nI’m off to go work in my preferred environment…\nOh also, how cool is it that the Quarto document ran both R & Python in one file?!"
  },
  {
    "objectID": "posts/2023-06-15/index.html",
    "href": "posts/2023-06-15/index.html",
    "title": "Blog Reconstruction",
    "section": "",
    "text": "Hi, welcome to my site!\nI recently finished grad school, moved states, and got married. Life has been hectic and not allowed time for public work. I’m hoping to be able to tackle some public projects moving forward.\nI’m particularly excited about new developments with the Apache Arrow project, and how to bridge the gap with R and Python users. I have ADHD, which means I think in a non-linear and sporadic fashion. R will always be my true love for data analysis, with non-standard evaluation and emphasis by the Posit team on designing their API’s to be a fluid conversation with data.\nI have love for python too. Python was the first language I picked up. It taught me the beauty of understanding another way of thinking (something I again rediscovered learning about octopuses). I never excelled in math in the traditional school setting, but being able to define functions and interactively test different inputs really connected me in a way that greek letters and chalkboard never could. This is especially true for linear algebra. I don’t know that I would have any understanding if it wasn’t for numpy.\nThis is why I’m excited about polars. I love numpy and the optimization and syntax, however, it did not hold up well as a pandas backend as data grew more complex. Polars offers a syntax that requires less cognitive overhead, and the backed power of arrow and rust.\nAt work, I use these tools to be able to work in the R ecosystem for data exploration, which works for me. Additionally, I’m able to leverage arrow to convert back to python for our team’s development and machine learning environment.\nMy first (new) post will be detailing what that workflow looks like, and how easy it is to work within Quarto.\nIn the meantime, I left one (1) silly post about fantasy football. Mainly because I run that script once a year and still need it. Plus it’s always neat to be able to look back and see how far you’ve come."
  },
  {
    "objectID": "posts/2023-06-18-blog-reconstruction/index.html",
    "href": "posts/2023-06-18-blog-reconstruction/index.html",
    "title": "Blank Canvas",
    "section": "",
    "text": "Hi, welcome to my site!\nI recently finished grad school, moved states, and got married. Life has been hectic and not allowed time for public work. I’m hoping to be able to tackle some public projects moving forward.\nI’m particularly excited about new developments with the Apache Arrow project, and how to bridge the gap with R and Python users. I have ADHD, which means I think in a non-linear and sporadic fashion. R will always be my true love for data analysis, with non-standard evaluation and emphasis by the Posit team on designing their API’s to be a fluid conversation with data.\nI have love for python too. Python was the first language I picked up. It taught me the beauty of understanding another way of thinking (something I again rediscovered learning about octopuses, one of my favorite animals). I never excelled in math in the traditional school setting, but being able to define functions and interactively test different inputs really connected me in a way that greek letters and a chalkboard never could. This is especially true for linear algebra. I don’t know that I would have any understanding if it wasn’t for numpy.\nAt work, I use these tools to be able to work in the R ecosystem for data exploration, which works for me. Additionally, I leverage arrow to convert data back to python for our team’s development and machine learning environment.\nMy first (new) post will be detailing what that workflow looks like, and how easy it is to work within Quarto.\nIn the meantime, I left one (1) silly post about fantasy football. Mainly because I run that script once a year and still need it. Plus it’s always neat to be able to look back and see how far you’ve come."
  },
  {
    "objectID": "posts/2023-06-19-py-to-r/index.html#support-all-ways-of-thinking",
    "href": "posts/2023-06-19-py-to-r/index.html#support-all-ways-of-thinking",
    "title": "Arrow, Python, & R",
    "section": "Support All Ways of Thinking",
    "text": "Support All Ways of Thinking\nWhat is the reasoning behind this post? I love what I do, and can get an abundance of energy from projects that I’m working on. That being said, it’s incredibly frustrating using a specific tool, when I would enjoy and work faster in another tool.\n\nInsert Tired Python vs R Debate\nNothing will earn an auto-mute from me on Twitter faster than seeing a Python vs R debate. They are both fantastic tools, and each have their own strengths and weaknesses. What I find most important is to focus on how they can present themselves to a particular user. A feature I see as beneficial from R could be detrimental to another user, for a myriad of reasons.\nI support that setting up infrastructure which enables data professionals to use their tool of choice will help flourish the flow of ideas and analysis. Especially early on in the data exploration phase."
  },
  {
    "objectID": "posts/2023-08-09-reactable/index.html",
    "href": "posts/2023-08-09-reactable/index.html",
    "title": "Reactable Demo",
    "section": "",
    "text": "Hello"
  },
  {
    "objectID": "posts/2023-06-18-blog-reconstruction/index.html#intorduction",
    "href": "posts/2023-06-18-blog-reconstruction/index.html#intorduction",
    "title": "Blank Canvas",
    "section": "",
    "text": "Hi, welcome to my site!\nI recently finished grad school, moved states, and got married. Life has been hectic and not allowed time for public work. I’m hoping to be able to tackle some public projects moving forward.\nI’m particularly excited about new developments with the Apache Arrow project, and how to bridge the gap with R and Python users. I have ADHD, which means I think in a non-linear and sporadic fashion. R will always be my true love for data analysis, with non-standard evaluation and emphasis by the Posit team on designing their API’s to be a fluid conversation with data.\nI have love for python too. Python was the first language I picked up. It taught me the beauty of understanding another way of thinking (something I again rediscovered learning about octopuses, one of my favorite animals). I never excelled in math in the traditional school setting, but being able to define functions and interactively test different inputs really connected me in a way that greek letters and a chalkboard never could. This is especially true for linear algebra. I don’t know that I would have any understanding if it wasn’t for numpy.\nAt work, I use these tools to be able to work in the R ecosystem for data exploration, which works for me. Additionally, I leverage arrow to convert data back to python for our team’s development and machine learning environment.\nMy first (new) post will be detailing what that workflow looks like, and how easy it is to work within Quarto.\nIn the meantime, I left one (1) silly post about fantasy football. Mainly because I run that script once a year and still need it. Plus it’s always neat to be able to look back and see how far you’ve come."
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About",
    "section": "",
    "text": "I’m a data analyst living in Colorado Springs, currently working in product analytics. Previously I worked in aviation simulating airport facilities.\nMy passion is in making processes more efficient. Whether that is ETL pipelines to expose better analytics tables, visualizations that save time on investigating or filtering, or automating tedious steps in the workflow. I’m a proponent of working with what tool and environment works best for you, and enjoy designing solutions to be inclusive for everyone.\nIn my free time I love exploring the outdoors, climbing, yoga, and spending time with my wife, dogs, and evil cat."
  },
  {
    "objectID": "posts/2024-06-01-flight-data/index.html",
    "href": "posts/2024-06-01-flight-data/index.html",
    "title": "Modeling Airports",
    "section": "",
    "text": "I was once asked during my time working with Southwest Airlines how I got into aviation. At the time I never thought of myself associated with aviation, only data. After leaving, I soon realized that I loved aviation and working for airlines. Okay, maybe not all of the emissions, harm to environment, and how they probably lobby against trains… but I do love all the applications of simulation! Luckily there is the nycflights13 publicly available to work with.\nEverything I will discuss is well known and industry standard. Transportation Security Administration (TSA) and the International Association of Baggage System Companies (IABSC) have a lot of this documented, which you can read about here.\nNOTE: I intend to have a series of different topics in simulation, with this being a foundation introduction. When I refer to things out of scope, my plan is to cover it in a later post. Simulation covers topics of uncertainty while also providing data engineering challenges due to the size of data generated. Maybe we can leverage our beloved DuckDB in a future post too. Let’s get to started!\n\n\nThe foundation for anything we do is the flight schedule. Here is a snapshot of the flight schedule available from the nycflights13 package.\n\n\nCode\nlibrary(tidyverse)\nlibrary(nycflights13)\nlibrary(reactable)\n\n\nflights |&gt; \n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\ntime_hour\n\n\n\n\n2013\n1\n1\n517\n515\n2\n830\n819\n11\nUA\n1545\nN14228\nEWR\nIAH\n227\n1400\n5\n15\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n533\n529\n4\n850\n830\n20\nUA\n1714\nN24211\nLGA\nIAH\n227\n1416\n5\n29\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n542\n540\n2\n923\n850\n33\nAA\n1141\nN619AA\nJFK\nMIA\n160\n1089\n5\n40\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n544\n545\n-1\n1004\n1022\n-18\nB6\n725\nN804JB\nJFK\nBQN\n183\n1576\n5\n45\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n554\n600\n-6\n812\n837\n-25\nDL\n461\nN668DN\nLGA\nATL\n116\n762\n6\n0\n2013-01-01 06:00:00\n\n\n2013\n1\n1\n554\n558\n-4\n740\n728\n12\nUA\n1696\nN39463\nEWR\nORD\n150\n719\n5\n58\n2013-01-01 05:00:00\n\n\n\n\n\n\n\n\n\nWe have flight counts from every airline for the 3 New York airports throughout 2013. I picked flights from my birthday, June 27, to show flight counts for each station. Certain carriers have a dominant market share, and some carriers do not utilize the station at all.\n\n\nCode\njune27_flights &lt;- flights |&gt; \n  mutate(dep_dttm = time_hour + minutes(minute)) |&gt; \n  filter(day == 27 , month == 6)\n\njune27_flights |&gt; \n  count(carrier, origin) |&gt; \n  pivot_wider(id_cols = carrier, names_from = origin, values_from = n) |&gt; \n  mutate(across(everything(), \\(x) replace_na(x, 0))) |&gt; \n  arrange(-LGA) |&gt; \n  reactable(\n    bordered = TRUE\n  )\n\n\n\n\n\n\n\n\n\n\n\nAn arrival curve represents the distribution of passenger arrivals before the departure time. Several factors influence how early or close to departure passengers arrive, but these can generally be categorized into three main distributions:\n\nDomestic Before 9am - Domestic flights departing before 9am have tighter distributions, as passengers find it harder to arrive extremely early.\nDomestic After 9am - Off-peak domestic flights (departing during the day or evening) allow passengers ample time to arrive early.\nInternational - Non-domestic flights, often involving more luggage, customs, and higher stress levels, typically see passengers arriving much earlier.\n\nThe Planning Guidelines and Design Standards for TSA provide an arrival curve on page 91. As an LLM / AI pessimist, I have to give credit when it is due. To get the arrival curve I copied the table values from the pdf and asked OpenAI’s chatGPT to provide code to replicate the table, which worked seamlessly.\nThe comparison of the three distributions reveals distinct patterns. Domestic flights have a mode around an hour before departure. In contrast, international flights show passengers arriving much earlier, with a longer tail towards earlier arrivals.\n\n\nCode\nstation_colors &lt;- c('#e41a1c', '#377eb8', '#4daf4a')\nstation_colors &lt;- c('#66c2a5', '#fc8d62', '#8da0cb')\n\ndf &lt;- tibble(\n  minutes_prior = rev(c(\"&gt;240\", \"240\", \"230\", \"220\", \"210\", \"200\", \"190\", \"180\", \"170\", \"160\", \"150\", \"140\", \"130\", \"120\", \"110\", \"100\", \"90\", \"80\", \"70\", \"60\", \"50\", \"40\", \"30\", \"20\", \"10\")),\n  peak_domestic_8am = c(0.80, 0.26, 0.42, 1.10, 3.08, 6.71, 10.34, 12.87, 13.54, \n                        12.79, 11.21, 8.70, 6.13, 4.11, 2.66, 1.69, 1.10, 0.72, \n                        0.46, 0.32, 0.22, 0.15, 0.11, 0.08, 0.41),\n  off_peak_domestic = c(0.06, 0.30, 0.48, 0.98, 2.10, 4.03, 6.19, 8.16, 9.59, \n                       10.25, 10.08, 9.25, 7.95, 6.44, 5.09, 3.94, 3.06, 2.36, \n                       1.83, 1.43, 1.14, 0.92, 0.74, 0.62, 3.01),\n  international = c(0.22, 0.11, 0.15, 0.28, 0.61, 1.32, 3.08, 5.13, 7.37, \n                   8.93, 10.28, 10.69, 9.75, 8.40, 7.12, 5.74, 4.75, 3.81, \n                   2.92, 2.17, 1.62, 1.19, 0.90, 0.71, 2.77)\n)\n\narrival_curve &lt;- df |&gt; \n    mutate(x = str_extract(minutes_prior, '\\\\d+') |&gt; as.numeric()) |&gt; \n  group_by(minutes_prior = x) |&gt; \n  summarise(\n    across(everything(), sum)\n  ) |&gt; \n  select(-x) |&gt; \n  pivot_longer(-minutes_prior) |&gt; \n  mutate(value = value / 100)\n\narrival_curve |&gt; \n  ggplot(aes(minutes_prior, value, color = name)) +\n  geom_point() +\n  geom_line(linewidth = 2) +\n  labs(\n    title = 'Arrival Curve',\n    subtitle = 'The distribution of minutes prior to departure that a passenger will arrive',\n    x = 'Minutes Prior',\n    y = NULL,\n    color = NULL,\n    fill = NULL,\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_x_reverse() +\n  theme(legend.position = 'top') \n\n\n\n\n\n\n\n\nOur goal is to simulate arrivals from this distribution to optimize airport operations and planning. The data is binned into 10-minute intervals, which is appropriate for many use cases.\nTo simulate arrivals:\n\nSample from Bins: Using the table of percentages of passengers arriving in each 10-minute bin, we can sample with replacement from the minutes_prior column, guided by the probabilities from the arrival curve.\nKernel Density Estimate (KDE): Calculate a KDE to create a continuous distribution.\n\nNote: The KDE may produce some negative values. Although these occurrences are rare, negative times are not possible. Therefore, I filtered out rows with minutes_prior &lt; 0 and recalculated the percentages, dividing by the sum of all valid percentages for that group.”\n\n\nCode\nset.seed(0527)\n\narrival_samples &lt;- arrival_curve |&gt; \n  group_by(name) |&gt; \n  nest() |&gt; \n  ungroup() |&gt; \n  mutate(\n    samples = map(.x = data, ~sample(.x$minutes_prior, 1000, replace = TRUE, prob = .x$value))\n  ) |&gt; \n  mutate(\n    d = map(samples, ~density(.x)),\n    density_x = map(d, \"x\"),\n    density_y = map(d, \"y\"),\n    dens = tibble(dens_x = map(d, \"x\"), dens_y = map(d, \"y\"))\n  ) |&gt; \n  ungroup() \n\narrival_kde &lt;- arrival_samples |&gt; \n  select(name, starts_with('density')) |&gt; \n  unnest(c(density_x, density_y)) |&gt; \n  filter(density_x &gt; 0) |&gt; \n  group_by(name) |&gt; \n  mutate(y = density_y / sum(density_y)) |&gt;\n  select(name, minutes_prior = density_x, perc = y) |&gt; \n  ungroup()\n\narrival_kde |&gt; \n  ggplot(aes(minutes_prior, perc, color = name)) +\n  geom_line(linewidth = 2) + \n  labs(\n    title = 'KDE of Arrival Curve',\n    x = 'Minutes Prior',\n    y = NULL,\n    color = NULL,\n    fill = NULL,\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_x_reverse() + \n  theme(legend.position = 'top')\n\n\n\n\n\n\n\n\nIn aviation planning, estimating passenger numbers for a flight involves considering two primary factors:\n\nLoad Factor: This refers to the percentage of seats occupied on a flight.\nOriginating Factor: This factor indicates the proportion of passengers whose journey originates at a particular station.\n\nFor calculation purposes, let’s consider an example with 100 seats available on a flight, an 80% load factor, and 90% of passengers originating from the local station.\nThe formula for deriving the expected number of passengers is:\nExpected Passengers = Seats × Load Factor × Originating Factor\nSubstituting the given values into the formula:\nExpected Passengers = 100 × 0.80 × 0.90\nExpected Passengers = 72\nSo, in this scenario, we would expect approximately 72 passengers for the flight.\n\n\n\nTypically, airlines maintain their own datasets on various factors affecting flight operations. These include market dynamics, flight length, seasonality components, and other variables that influence load factors.\nFor this, we will use the table provided in the PGDS Study on page 395. This gives a breakdown of the average load factor for each flight. Additionally it contains two other factors used later on, which are:\n\nChecked Bag Factor: This represents the percentage of passengers checking bags.\nAvg Number of Bags: This figure indicates the expected number of bags per passenger after accounting for the previously mentioned load factors.\n\nWe’ll use this table generate more sampling data.\n\n\nCode\n# Define the data\noperator_name &lt;- c(\"Continental Airlines\", \"Alaska Airlines\", \"America West Airlines (domestic destinations)\", \n                   \"United Airlines\", \"XX Airlines\", \"SkyWest Airlines\", \"American Airlines\", \n                   \"JetBlue Airways\", \"Delta Air Lines\", \"America West Airlines (Mexican destinations)\", \n                   \"Aloha Airlines\", \"Horizon Air\", \"Mesa Airlines\", \"ATA Airlines\", \n                   \"United Express/SkyWest Airlines\")\noperator_code &lt;- c(\"CO\", \"AS\", \"HP\", \"UA\", \"XX\", \"OO\", \"AA\", \"B6\", \"DL\", \"HP\", \"AQ\", \"QX\", \"YV\", \"TZ\", \"A296\")\nload_factor &lt;- c(96, 98, 83, 85, 77, 91, 98, 90, 89, 83, 85, 60, 85, 85, 91)\n\npercent_of_parties_checking_pre_gate &lt;- c(75, 80, 84, 45, 34, 79, 90, 90, 92, 100, 97, 77, 51, 64, 66)\naverage_number_of_checked_bags_per_passenger &lt;- c(0.79, 0.71, 0.68, 0.87, 0.92, 0.91, 0.71, 0.90, 0.98, 1.30, 1.30, 0.95, 0.96, 1.23, 0.87)\n\n# Create the data frame\noperator_data &lt;- tibble(airline = operator_name,\n                            carrier = operator_code,\n                            load_factor = load_factor,\n                            check_bag_factor = percent_of_parties_checking_pre_gate,\n                            avg_num_bags = average_number_of_checked_bags_per_passenger) |&gt; \n  mutate(across(contains('factor'), \\(x) x/100))\n\noperator_data\n\n\n\n\n\n\n\n\n\n\n\n\n\nairline\ncarrier\nload_factor\ncheck_bag_factor\navg_num_bags\n\n\n\n\nContinental Airlines\nCO\n0.96\n0.75\n0.79\n\n\nAlaska Airlines\nAS\n0.98\n0.80\n0.71\n\n\nAmerica West Airlines (domestic destinations)\nHP\n0.83\n0.84\n0.68\n\n\nUnited Airlines\nUA\n0.85\n0.45\n0.87\n\n\nXX Airlines\nXX\n0.77\n0.34\n0.92\n\n\nSkyWest Airlines\nOO\n0.91\n0.79\n0.91\n\n\nAmerican Airlines\nAA\n0.98\n0.90\n0.71\n\n\nJetBlue Airways\nB6\n0.90\n0.90\n0.90\n\n\nDelta Air Lines\nDL\n0.89\n0.92\n0.98\n\n\nAmerica West Airlines (Mexican destinations)\nHP\n0.83\n1.00\n1.30\n\n\nAloha Airlines\nAQ\n0.85\n0.97\n1.30\n\n\nHorizon Air\nQX\n0.60\n0.77\n0.95\n\n\nMesa Airlines\nYV\n0.85\n0.51\n0.96\n\n\nATA Airlines\nTZ\n0.85\n0.64\n1.23\n\n\nUnited Express/SkyWest Airlines\nA296\n0.91\n0.66\n0.87\n\n\n\n\n\n\n\n\n\nIn this section, we’ll start by modeling passenger numbers under the assumption that all passengers are local. While this assumption may oversimplify reality, it serves as a conservative estimate for our analysis. With our data focusing on New York, US stations, this follows certain characteristics of domestic flights, particularly their tendency to follow daylight and fly from east to west.\nDomestic flights originating from the East Coast have high originating factors due to the network’s construction.\nOnce we establish the basics of passenger modeling under this assumption, we can explore alternative solutions to refine our approach and accommodate more complex scenarios.\n\n\n\nJoin seats of aircraft and factors to flight schedule\nDetermine passengers for each flight\nDistribute the passengers across generic arrival curve (10 minute increment)\n\nThe arrival curve used here is just the 10 minute increment arrival curve provided, and airports is part of the nycflights13 package. We will use this to determine domestic/international stations. The result is the flight schedule with an arrival curve joined to each row.\n\n\nCode\narrival_join_10_min &lt;- arrival_curve |&gt; \n  pivot_wider(names_from = minutes_prior, values_from = value) |&gt; \n  mutate(\n    peak = name == 'peak_domestic_8am',\n    domestic = !str_detect(name, 'international')\n    ) \n\nairports &lt;- airports |&gt; \n  mutate(domestic = str_detect(tzone, 'America'))\n\njune27_base &lt;- june27_flights |&gt; \n  left_join(planes |&gt; select(tailnum, seats), by = c('tailnum')) |&gt; \n  left_join(operator_data, by = c('carrier')) |&gt; \n  left_join(airports, by =c('dest' = 'faa')) |&gt; \n  mutate(\n    peak = sched_dep_time &lt;= 800, \n    # use median values for missing airline data\n    across(c(contains('factor'), avg_num_bags ), \\(x) replace_na(x, median(x, na.rm = TRUE)))\n  ) |&gt; \n  mutate(\n    passengers = round(seats * load_factor)\n  ) |&gt; \n  left_join(\n    # 10 minute arrival curve\n    arrival_join_10_min,\n    by = c('peak', 'domestic')\n  ) |&gt; \n  select(carrier, origin, dest, dep_dttm, passengers, `10`:`240`)\n\n\njune27_base |&gt; \n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarrier\norigin\ndest\ndep_dttm\npassengers\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n170\n180\n190\n200\n210\n220\n230\n240\n\n\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n6e-04\n0.003\n0.0048\n0.0098\n0.021\n0.0403\n0.0619\n0.0816\n0.0959\n0.1025\n0.1008\n0.0925\n0.0795\n0.0644\n0.0509\n0.0394\n0.0306\n0.0236\n0.0183\n0.0143\n0.0114\n0.0092\n0.0074\n0.0363\n\n\nB6\nJFK\nMCO\n2013-06-27 21:46:00\n18\n6e-04\n0.003\n0.0048\n0.0098\n0.021\n0.0403\n0.0619\n0.0816\n0.0959\n0.1025\n0.1008\n0.0925\n0.0795\n0.0644\n0.0509\n0.0394\n0.0306\n0.0236\n0.0183\n0.0143\n0.0114\n0.0092\n0.0074\n0.0363\n\n\nB6\nJFK\nFLL\n2013-06-27 21:55:00\n180\n6e-04\n0.003\n0.0048\n0.0098\n0.021\n0.0403\n0.0619\n0.0816\n0.0959\n0.1025\n0.1008\n0.0925\n0.0795\n0.0644\n0.0509\n0.0394\n0.0306\n0.0236\n0.0183\n0.0143\n0.0114\n0.0092\n0.0074\n0.0363\n\n\nB6\nJFK\nBUF\n2013-06-27 21:10:00\n18\n6e-04\n0.003\n0.0048\n0.0098\n0.021\n0.0403\n0.0619\n0.0816\n0.0959\n0.1025\n0.1008\n0.0925\n0.0795\n0.0644\n0.0509\n0.0394\n0.0306\n0.0236\n0.0183\n0.0143\n0.0114\n0.0092\n0.0074\n0.0363\n\n\nUA\nEWR\nCLE\n2013-06-27 20:21:00\n127\n6e-04\n0.003\n0.0048\n0.0098\n0.021\n0.0403\n0.0619\n0.0816\n0.0959\n0.1025\n0.1008\n0.0925\n0.0795\n0.0644\n0.0509\n0.0394\n0.0306\n0.0236\n0.0183\n0.0143\n0.0114\n0.0092\n0.0074\n0.0363\n\n\nB6\nJFK\nBOS\n2013-06-27 23:00:00\n18\n6e-04\n0.003\n0.0048\n0.0098\n0.021\n0.0403\n0.0619\n0.0816\n0.0959\n0.1025\n0.1008\n0.0925\n0.0795\n0.0644\n0.0509\n0.0394\n0.0306\n0.0236\n0.0183\n0.0143\n0.0114\n0.0092\n0.0074\n0.0363\n\n\n\n\n\n\n\n\n\n\nFor the simple approach, all we will do is distribute the passenger vector across the columns of the arrival curve percentages.\n\n\n\nMultiply passenger column by all of the arrival curve columns\nReshape the dataframe to a longer format so that each row represents a 10-minute interval per flight.\nSubtract that minute value from the departure datetime\nGroup by the new model time and aggregate the sum of passengers.\n\nNote: The departure times are in minute granularity. We group them into 10-minute bins to align with the basic arrival curve.\n\n\nCode\njune27_long &lt;- june27_base |&gt; \n  mutate(dep_dttm = if_else(minute(dep_dttm) %% 10 != 0 , dep_dttm - minutes(minute(dep_dttm) %% 10), dep_dttm)) |&gt; \n  mutate(across(`10`:`240`, \\(x) x * passengers)) |&gt; \n  pivot_longer(cols = `10`:`240`, names_to = 'minutes_prior', values_to = 'exp_passengers') |&gt; \n  mutate(\n    model_time = dep_dttm - minutes(minutes_prior)\n  )\n\njune27_long |&gt; \n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarrier\norigin\ndest\ndep_dttm\npassengers\nminutes_prior\nexp_passengers\nmodel_time\n\n\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n10\n0.0762\n2013-06-27 20:00:00\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n20\n0.3810\n2013-06-27 19:50:00\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n30\n0.6096\n2013-06-27 19:40:00\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n40\n1.2446\n2013-06-27 19:30:00\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n50\n2.6670\n2013-06-27 19:20:00\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n60\n5.1181\n2013-06-27 19:10:00\n\n\n\n\n\n\nCode\njune27_long |&gt; \n  group_by(origin, model_time) |&gt; \n  summarise(\n    expected_passengers = sum(exp_passengers, na.rm = TRUE),\n    .groups = 'drop'\n  ) |&gt; \n  ggplot(aes(model_time, expected_passengers)) +\n  geom_line(linewidth = 2, aes(color = origin)) +\n  facet_wrap(~origin, ncol = 1) +\n  labs(\n    title = 'Expected Passengers',\n    x = \"Time\",\n    y = \"Expected passengers\"\n  ) +\n  scale_color_manual(values = station_colors) +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nTo see the lagging effect of passengers arrivals relative to the departure time, you can visualize the passenger demand profile against the distribution of flights throughout the day.\n\n\nCode\njune27_long |&gt; \n  group_by(origin, model_time) |&gt; \n  summarise(\n    expected_passengers = sum(exp_passengers, na.rm = TRUE),\n    .groups = 'drop'\n  ) |&gt; \n  left_join(\n    june27_base |&gt;   \n      mutate(dep_dttm = if_else(minute(dep_dttm) %% 10 != 0 , dep_dttm - minutes(minute(dep_dttm) %% 10), dep_dttm)) |&gt; \n      count(dep_dttm, origin),\n    by = c('model_time' = 'dep_dttm', 'origin')\n  ) |&gt; \n  mutate(n = n * 50) |&gt; \n  #filter(origin == 'LGA') |&gt; \n  ggplot(aes(model_time, color = origin, fill = origin)) +\n  geom_col(aes(y=n), alpha = 0.26) +\n  geom_line(aes(y=expected_passengers), linewidth = 2) +\n  scale_y_continuous(sec.axis = sec_axis(~./50, name = \"Flight Count\")) +\n  facet_wrap(~origin, ncol = 1) +\n  labs(\n    title = 'Distirbution of Passengers and Flights throughout the day',\n    y = 'Expected Passenger Demand',\n    x = 'Time',\n    color = NULL,\n    fill = NULL\n  ) +\n  theme(\n    legend.position = 'none',\n    plot.title = element_text(face = 'bold', size = 16)\n  ) +\n  scale_color_manual(values = station_colors)+\n  scale_fill_manual(values = station_colors)\n\n\n\n\n\n\n\n\n\nWhat we did is the bare minimum to model passengers, and later bags, throughout the day. This approach is likely used in the PGDS study, and linear calculations are in my opinion overused in modeling facilities today.\n\n\nEvery input can be sampled and simulated, and there are tradeoffs to consider with each approach. Using linear calculations is straightforward; for instance, you can easily explain that a flat load factor was used. However, this method falls short when trying to understand tail-end outcomes or disaster situations.\nWe’ll start by simulating passengers from the arrival curve in the next post."
  },
  {
    "objectID": "posts/2024-06-01-flight-data/index.html#aviation-and-simulation",
    "href": "posts/2024-06-01-flight-data/index.html#aviation-and-simulation",
    "title": "Modeling Airports",
    "section": "",
    "text": "I was once asked during my time working with Southwest Airlines how I got into aviation. At the time I never thought of myself associated with aviation, only data. After leaving, I soon realized that I loved aviation and working for airlines. Okay, maybe not all of the emissions, harm to environment, and how they probably lobby against trains… but I do love all the applications of simulation! Luckily there is the nycflights13 publicly available to work with.\nEverything I will discuss is well known and industry standard. Transportation Security Administration (TSA) and the International Association of Baggage System Companies (IABSC) have a lot of this documented, which you can read about here.\nNOTE: I intend to have a series of different topics in simulation, with this being a foundation introduction. When I refer to things out of scope, my plan is to cover it in a later post. Simulation covers topics of uncertainty while also providing data engineering challenges due to the size of data generated. Maybe we can leverage our beloved DuckDB in a future post too. Let’s get to started!\n\n\nThe foundation for anything we do is the flight schedule. Here is a snapshot of the flight schedule available from the nycflights13 package.\n\n\nCode\nlibrary(tidyverse)\nlibrary(nycflights13)\nlibrary(reactable)\n\n\nflights |&gt; \n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\ntime_hour\n\n\n\n\n2013\n1\n1\n517\n515\n2\n830\n819\n11\nUA\n1545\nN14228\nEWR\nIAH\n227\n1400\n5\n15\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n533\n529\n4\n850\n830\n20\nUA\n1714\nN24211\nLGA\nIAH\n227\n1416\n5\n29\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n542\n540\n2\n923\n850\n33\nAA\n1141\nN619AA\nJFK\nMIA\n160\n1089\n5\n40\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n544\n545\n-1\n1004\n1022\n-18\nB6\n725\nN804JB\nJFK\nBQN\n183\n1576\n5\n45\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n554\n600\n-6\n812\n837\n-25\nDL\n461\nN668DN\nLGA\nATL\n116\n762\n6\n0\n2013-01-01 06:00:00\n\n\n2013\n1\n1\n554\n558\n-4\n740\n728\n12\nUA\n1696\nN39463\nEWR\nORD\n150\n719\n5\n58\n2013-01-01 05:00:00\n\n\n\n\n\n\n\n\n\nWe have flight counts from every airline for the 3 New York airports throughout 2013. I picked flights from my birthday, June 27, to show flight counts for each station. Certain carriers have a dominant market share, and some carriers do not utilize the station at all.\n\n\nCode\njune27_flights &lt;- flights |&gt; \n  mutate(dep_dttm = time_hour + minutes(minute)) |&gt; \n  filter(day == 27 , month == 6)\n\njune27_flights |&gt; \n  count(carrier, origin) |&gt; \n  pivot_wider(id_cols = carrier, names_from = origin, values_from = n) |&gt; \n  mutate(across(everything(), \\(x) replace_na(x, 0))) |&gt; \n  arrange(-LGA) |&gt; \n  reactable(\n    bordered = TRUE\n  )\n\n\n\n\n\n\n\n\n\n\n\nAn arrival curve represents the distribution of passenger arrivals before the departure time. Several factors influence how early or close to departure passengers arrive, but these can generally be categorized into three main distributions:\n\nDomestic Before 9am - Domestic flights departing before 9am have tighter distributions, as passengers find it harder to arrive extremely early.\nDomestic After 9am - Off-peak domestic flights (departing during the day or evening) allow passengers ample time to arrive early.\nInternational - Non-domestic flights, often involving more luggage, customs, and higher stress levels, typically see passengers arriving much earlier.\n\nThe Planning Guidelines and Design Standards for TSA provide an arrival curve on page 91. As an LLM / AI pessimist, I have to give credit when it is due. To get the arrival curve I copied the table values from the pdf and asked OpenAI’s chatGPT to provide code to replicate the table, which worked seamlessly.\nThe comparison of the three distributions reveals distinct patterns. Domestic flights have a mode around an hour before departure. In contrast, international flights show passengers arriving much earlier, with a longer tail towards earlier arrivals.\n\n\nCode\nstation_colors &lt;- c('#e41a1c', '#377eb8', '#4daf4a')\nstation_colors &lt;- c('#66c2a5', '#fc8d62', '#8da0cb')\n\ndf &lt;- tibble(\n  minutes_prior = rev(c(\"&gt;240\", \"240\", \"230\", \"220\", \"210\", \"200\", \"190\", \"180\", \"170\", \"160\", \"150\", \"140\", \"130\", \"120\", \"110\", \"100\", \"90\", \"80\", \"70\", \"60\", \"50\", \"40\", \"30\", \"20\", \"10\")),\n  peak_domestic_8am = c(0.80, 0.26, 0.42, 1.10, 3.08, 6.71, 10.34, 12.87, 13.54, \n                        12.79, 11.21, 8.70, 6.13, 4.11, 2.66, 1.69, 1.10, 0.72, \n                        0.46, 0.32, 0.22, 0.15, 0.11, 0.08, 0.41),\n  off_peak_domestic = c(0.06, 0.30, 0.48, 0.98, 2.10, 4.03, 6.19, 8.16, 9.59, \n                       10.25, 10.08, 9.25, 7.95, 6.44, 5.09, 3.94, 3.06, 2.36, \n                       1.83, 1.43, 1.14, 0.92, 0.74, 0.62, 3.01),\n  international = c(0.22, 0.11, 0.15, 0.28, 0.61, 1.32, 3.08, 5.13, 7.37, \n                   8.93, 10.28, 10.69, 9.75, 8.40, 7.12, 5.74, 4.75, 3.81, \n                   2.92, 2.17, 1.62, 1.19, 0.90, 0.71, 2.77)\n)\n\narrival_curve &lt;- df |&gt; \n    mutate(x = str_extract(minutes_prior, '\\\\d+') |&gt; as.numeric()) |&gt; \n  group_by(minutes_prior = x) |&gt; \n  summarise(\n    across(everything(), sum)\n  ) |&gt; \n  select(-x) |&gt; \n  pivot_longer(-minutes_prior) |&gt; \n  mutate(value = value / 100)\n\narrival_curve |&gt; \n  ggplot(aes(minutes_prior, value, color = name)) +\n  geom_point() +\n  geom_line(linewidth = 2) +\n  labs(\n    title = 'Arrival Curve',\n    subtitle = 'The distribution of minutes prior to departure that a passenger will arrive',\n    x = 'Minutes Prior',\n    y = NULL,\n    color = NULL,\n    fill = NULL,\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_x_reverse() +\n  theme(legend.position = 'top') \n\n\n\n\n\n\n\n\nOur goal is to simulate arrivals from this distribution to optimize airport operations and planning. The data is binned into 10-minute intervals, which is appropriate for many use cases.\nTo simulate arrivals:\n\nSample from Bins: Using the table of percentages of passengers arriving in each 10-minute bin, we can sample with replacement from the minutes_prior column, guided by the probabilities from the arrival curve.\nKernel Density Estimate (KDE): Calculate a KDE to create a continuous distribution.\n\nNote: The KDE may produce some negative values. Although these occurrences are rare, negative times are not possible. Therefore, I filtered out rows with minutes_prior &lt; 0 and recalculated the percentages, dividing by the sum of all valid percentages for that group.”\n\n\nCode\nset.seed(0527)\n\narrival_samples &lt;- arrival_curve |&gt; \n  group_by(name) |&gt; \n  nest() |&gt; \n  ungroup() |&gt; \n  mutate(\n    samples = map(.x = data, ~sample(.x$minutes_prior, 1000, replace = TRUE, prob = .x$value))\n  ) |&gt; \n  mutate(\n    d = map(samples, ~density(.x)),\n    density_x = map(d, \"x\"),\n    density_y = map(d, \"y\"),\n    dens = tibble(dens_x = map(d, \"x\"), dens_y = map(d, \"y\"))\n  ) |&gt; \n  ungroup() \n\narrival_kde &lt;- arrival_samples |&gt; \n  select(name, starts_with('density')) |&gt; \n  unnest(c(density_x, density_y)) |&gt; \n  filter(density_x &gt; 0) |&gt; \n  group_by(name) |&gt; \n  mutate(y = density_y / sum(density_y)) |&gt;\n  select(name, minutes_prior = density_x, perc = y) |&gt; \n  ungroup()\n\narrival_kde |&gt; \n  ggplot(aes(minutes_prior, perc, color = name)) +\n  geom_line(linewidth = 2) + \n  labs(\n    title = 'KDE of Arrival Curve',\n    x = 'Minutes Prior',\n    y = NULL,\n    color = NULL,\n    fill = NULL,\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_x_reverse() + \n  theme(legend.position = 'top')\n\n\n\n\n\n\n\n\nIn aviation planning, estimating passenger numbers for a flight involves considering two primary factors:\n\nLoad Factor: This refers to the percentage of seats occupied on a flight.\nOriginating Factor: This factor indicates the proportion of passengers whose journey originates at a particular station.\n\nFor calculation purposes, let’s consider an example with 100 seats available on a flight, an 80% load factor, and 90% of passengers originating from the local station.\nThe formula for deriving the expected number of passengers is:\nExpected Passengers = Seats × Load Factor × Originating Factor\nSubstituting the given values into the formula:\nExpected Passengers = 100 × 0.80 × 0.90\nExpected Passengers = 72\nSo, in this scenario, we would expect approximately 72 passengers for the flight.\n\n\n\nTypically, airlines maintain their own datasets on various factors affecting flight operations. These include market dynamics, flight length, seasonality components, and other variables that influence load factors.\nFor this, we will use the table provided in the PGDS Study on page 395. This gives a breakdown of the average load factor for each flight. Additionally it contains two other factors used later on, which are:\n\nChecked Bag Factor: This represents the percentage of passengers checking bags.\nAvg Number of Bags: This figure indicates the expected number of bags per passenger after accounting for the previously mentioned load factors.\n\nWe’ll use this table generate more sampling data.\n\n\nCode\n# Define the data\noperator_name &lt;- c(\"Continental Airlines\", \"Alaska Airlines\", \"America West Airlines (domestic destinations)\", \n                   \"United Airlines\", \"XX Airlines\", \"SkyWest Airlines\", \"American Airlines\", \n                   \"JetBlue Airways\", \"Delta Air Lines\", \"America West Airlines (Mexican destinations)\", \n                   \"Aloha Airlines\", \"Horizon Air\", \"Mesa Airlines\", \"ATA Airlines\", \n                   \"United Express/SkyWest Airlines\")\noperator_code &lt;- c(\"CO\", \"AS\", \"HP\", \"UA\", \"XX\", \"OO\", \"AA\", \"B6\", \"DL\", \"HP\", \"AQ\", \"QX\", \"YV\", \"TZ\", \"A296\")\nload_factor &lt;- c(96, 98, 83, 85, 77, 91, 98, 90, 89, 83, 85, 60, 85, 85, 91)\n\npercent_of_parties_checking_pre_gate &lt;- c(75, 80, 84, 45, 34, 79, 90, 90, 92, 100, 97, 77, 51, 64, 66)\naverage_number_of_checked_bags_per_passenger &lt;- c(0.79, 0.71, 0.68, 0.87, 0.92, 0.91, 0.71, 0.90, 0.98, 1.30, 1.30, 0.95, 0.96, 1.23, 0.87)\n\n# Create the data frame\noperator_data &lt;- tibble(airline = operator_name,\n                            carrier = operator_code,\n                            load_factor = load_factor,\n                            check_bag_factor = percent_of_parties_checking_pre_gate,\n                            avg_num_bags = average_number_of_checked_bags_per_passenger) |&gt; \n  mutate(across(contains('factor'), \\(x) x/100))\n\noperator_data\n\n\n\n\n\n\n\n\n\n\n\n\n\nairline\ncarrier\nload_factor\ncheck_bag_factor\navg_num_bags\n\n\n\n\nContinental Airlines\nCO\n0.96\n0.75\n0.79\n\n\nAlaska Airlines\nAS\n0.98\n0.80\n0.71\n\n\nAmerica West Airlines (domestic destinations)\nHP\n0.83\n0.84\n0.68\n\n\nUnited Airlines\nUA\n0.85\n0.45\n0.87\n\n\nXX Airlines\nXX\n0.77\n0.34\n0.92\n\n\nSkyWest Airlines\nOO\n0.91\n0.79\n0.91\n\n\nAmerican Airlines\nAA\n0.98\n0.90\n0.71\n\n\nJetBlue Airways\nB6\n0.90\n0.90\n0.90\n\n\nDelta Air Lines\nDL\n0.89\n0.92\n0.98\n\n\nAmerica West Airlines (Mexican destinations)\nHP\n0.83\n1.00\n1.30\n\n\nAloha Airlines\nAQ\n0.85\n0.97\n1.30\n\n\nHorizon Air\nQX\n0.60\n0.77\n0.95\n\n\nMesa Airlines\nYV\n0.85\n0.51\n0.96\n\n\nATA Airlines\nTZ\n0.85\n0.64\n1.23\n\n\nUnited Express/SkyWest Airlines\nA296\n0.91\n0.66\n0.87\n\n\n\n\n\n\n\n\n\nIn this section, we’ll start by modeling passenger numbers under the assumption that all passengers are local. While this assumption may oversimplify reality, it serves as a conservative estimate for our analysis. With our data focusing on New York, US stations, this follows certain characteristics of domestic flights, particularly their tendency to follow daylight and fly from east to west.\nDomestic flights originating from the East Coast have high originating factors due to the network’s construction.\nOnce we establish the basics of passenger modeling under this assumption, we can explore alternative solutions to refine our approach and accommodate more complex scenarios.\n\n\n\nJoin seats of aircraft and factors to flight schedule\nDetermine passengers for each flight\nDistribute the passengers across generic arrival curve (10 minute increment)\n\nThe arrival curve used here is just the 10 minute increment arrival curve provided, and airports is part of the nycflights13 package. We will use this to determine domestic/international stations. The result is the flight schedule with an arrival curve joined to each row.\n\n\nCode\narrival_join_10_min &lt;- arrival_curve |&gt; \n  pivot_wider(names_from = minutes_prior, values_from = value) |&gt; \n  mutate(\n    peak = name == 'peak_domestic_8am',\n    domestic = !str_detect(name, 'international')\n    ) \n\nairports &lt;- airports |&gt; \n  mutate(domestic = str_detect(tzone, 'America'))\n\njune27_base &lt;- june27_flights |&gt; \n  left_join(planes |&gt; select(tailnum, seats), by = c('tailnum')) |&gt; \n  left_join(operator_data, by = c('carrier')) |&gt; \n  left_join(airports, by =c('dest' = 'faa')) |&gt; \n  mutate(\n    peak = sched_dep_time &lt;= 800, \n    # use median values for missing airline data\n    across(c(contains('factor'), avg_num_bags ), \\(x) replace_na(x, median(x, na.rm = TRUE)))\n  ) |&gt; \n  mutate(\n    passengers = round(seats * load_factor)\n  ) |&gt; \n  left_join(\n    # 10 minute arrival curve\n    arrival_join_10_min,\n    by = c('peak', 'domestic')\n  ) |&gt; \n  select(carrier, origin, dest, dep_dttm, passengers, `10`:`240`)\n\n\njune27_base |&gt; \n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarrier\norigin\ndest\ndep_dttm\npassengers\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n170\n180\n190\n200\n210\n220\n230\n240\n\n\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n6e-04\n0.003\n0.0048\n0.0098\n0.021\n0.0403\n0.0619\n0.0816\n0.0959\n0.1025\n0.1008\n0.0925\n0.0795\n0.0644\n0.0509\n0.0394\n0.0306\n0.0236\n0.0183\n0.0143\n0.0114\n0.0092\n0.0074\n0.0363\n\n\nB6\nJFK\nMCO\n2013-06-27 21:46:00\n18\n6e-04\n0.003\n0.0048\n0.0098\n0.021\n0.0403\n0.0619\n0.0816\n0.0959\n0.1025\n0.1008\n0.0925\n0.0795\n0.0644\n0.0509\n0.0394\n0.0306\n0.0236\n0.0183\n0.0143\n0.0114\n0.0092\n0.0074\n0.0363\n\n\nB6\nJFK\nFLL\n2013-06-27 21:55:00\n180\n6e-04\n0.003\n0.0048\n0.0098\n0.021\n0.0403\n0.0619\n0.0816\n0.0959\n0.1025\n0.1008\n0.0925\n0.0795\n0.0644\n0.0509\n0.0394\n0.0306\n0.0236\n0.0183\n0.0143\n0.0114\n0.0092\n0.0074\n0.0363\n\n\nB6\nJFK\nBUF\n2013-06-27 21:10:00\n18\n6e-04\n0.003\n0.0048\n0.0098\n0.021\n0.0403\n0.0619\n0.0816\n0.0959\n0.1025\n0.1008\n0.0925\n0.0795\n0.0644\n0.0509\n0.0394\n0.0306\n0.0236\n0.0183\n0.0143\n0.0114\n0.0092\n0.0074\n0.0363\n\n\nUA\nEWR\nCLE\n2013-06-27 20:21:00\n127\n6e-04\n0.003\n0.0048\n0.0098\n0.021\n0.0403\n0.0619\n0.0816\n0.0959\n0.1025\n0.1008\n0.0925\n0.0795\n0.0644\n0.0509\n0.0394\n0.0306\n0.0236\n0.0183\n0.0143\n0.0114\n0.0092\n0.0074\n0.0363\n\n\nB6\nJFK\nBOS\n2013-06-27 23:00:00\n18\n6e-04\n0.003\n0.0048\n0.0098\n0.021\n0.0403\n0.0619\n0.0816\n0.0959\n0.1025\n0.1008\n0.0925\n0.0795\n0.0644\n0.0509\n0.0394\n0.0306\n0.0236\n0.0183\n0.0143\n0.0114\n0.0092\n0.0074\n0.0363\n\n\n\n\n\n\n\n\n\n\nFor the simple approach, all we will do is distribute the passenger vector across the columns of the arrival curve percentages.\n\n\n\nMultiply passenger column by all of the arrival curve columns\nReshape the dataframe to a longer format so that each row represents a 10-minute interval per flight.\nSubtract that minute value from the departure datetime\nGroup by the new model time and aggregate the sum of passengers.\n\nNote: The departure times are in minute granularity. We group them into 10-minute bins to align with the basic arrival curve.\n\n\nCode\njune27_long &lt;- june27_base |&gt; \n  mutate(dep_dttm = if_else(minute(dep_dttm) %% 10 != 0 , dep_dttm - minutes(minute(dep_dttm) %% 10), dep_dttm)) |&gt; \n  mutate(across(`10`:`240`, \\(x) x * passengers)) |&gt; \n  pivot_longer(cols = `10`:`240`, names_to = 'minutes_prior', values_to = 'exp_passengers') |&gt; \n  mutate(\n    model_time = dep_dttm - minutes(minutes_prior)\n  )\n\njune27_long |&gt; \n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarrier\norigin\ndest\ndep_dttm\npassengers\nminutes_prior\nexp_passengers\nmodel_time\n\n\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n10\n0.0762\n2013-06-27 20:00:00\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n20\n0.3810\n2013-06-27 19:50:00\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n30\n0.6096\n2013-06-27 19:40:00\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n40\n1.2446\n2013-06-27 19:30:00\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n50\n2.6670\n2013-06-27 19:20:00\n\n\nUA\nEWR\nMIA\n2013-06-27 20:10:00\n127\n60\n5.1181\n2013-06-27 19:10:00\n\n\n\n\n\n\nCode\njune27_long |&gt; \n  group_by(origin, model_time) |&gt; \n  summarise(\n    expected_passengers = sum(exp_passengers, na.rm = TRUE),\n    .groups = 'drop'\n  ) |&gt; \n  ggplot(aes(model_time, expected_passengers)) +\n  geom_line(linewidth = 2, aes(color = origin)) +\n  facet_wrap(~origin, ncol = 1) +\n  labs(\n    title = 'Expected Passengers',\n    x = \"Time\",\n    y = \"Expected passengers\"\n  ) +\n  scale_color_manual(values = station_colors) +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nTo see the lagging effect of passengers arrivals relative to the departure time, you can visualize the passenger demand profile against the distribution of flights throughout the day.\n\n\nCode\njune27_long |&gt; \n  group_by(origin, model_time) |&gt; \n  summarise(\n    expected_passengers = sum(exp_passengers, na.rm = TRUE),\n    .groups = 'drop'\n  ) |&gt; \n  left_join(\n    june27_base |&gt;   \n      mutate(dep_dttm = if_else(minute(dep_dttm) %% 10 != 0 , dep_dttm - minutes(minute(dep_dttm) %% 10), dep_dttm)) |&gt; \n      count(dep_dttm, origin),\n    by = c('model_time' = 'dep_dttm', 'origin')\n  ) |&gt; \n  mutate(n = n * 50) |&gt; \n  #filter(origin == 'LGA') |&gt; \n  ggplot(aes(model_time, color = origin, fill = origin)) +\n  geom_col(aes(y=n), alpha = 0.26) +\n  geom_line(aes(y=expected_passengers), linewidth = 2) +\n  scale_y_continuous(sec.axis = sec_axis(~./50, name = \"Flight Count\")) +\n  facet_wrap(~origin, ncol = 1) +\n  labs(\n    title = 'Distirbution of Passengers and Flights throughout the day',\n    y = 'Expected Passenger Demand',\n    x = 'Time',\n    color = NULL,\n    fill = NULL\n  ) +\n  theme(\n    legend.position = 'none',\n    plot.title = element_text(face = 'bold', size = 16)\n  ) +\n  scale_color_manual(values = station_colors)+\n  scale_fill_manual(values = station_colors)\n\n\n\n\n\n\n\n\n\nWhat we did is the bare minimum to model passengers, and later bags, throughout the day. This approach is likely used in the PGDS study, and linear calculations are in my opinion overused in modeling facilities today.\n\n\nEvery input can be sampled and simulated, and there are tradeoffs to consider with each approach. Using linear calculations is straightforward; for instance, you can easily explain that a flat load factor was used. However, this method falls short when trying to understand tail-end outcomes or disaster situations.\nWe’ll start by simulating passengers from the arrival curve in the next post."
  }
]