---
title: "Arrival Curve Sampling"
author: "Ryan Plain"
date: "2024-06-02"
categories: [Flights, R]
description: "Adding variance using the KDE of each arrival curve"
draft: false
format: 
  html: 
    code-fold: true
    code-summary: "Show the code"

---


## Flight Analytics

In this post, we continue our exploration of modeling passenger and baggage traffic in airports using publicly available data. To streamline our data processing, I created a simple R package [flightanalytics](https://github.com/rplain1/flightanalytics). This package helps to build dataframes from the [Planning Guidelines and Design Standards for TSA](https://iabsc.org/wp-content/uploads/2021/04/Planning-Guidelines-and-Design-Standards-for-Checked-Baggage-Inspection-Systems-V7.0.pdf) study. 

For now, we will use the arrival curve and some carrier-level aggregated passenger and bag data from the TSA document. The `flightanalytics` package is used for storing the logic required to build these tables from the PDF document. By consolidating this logic into a package, we can easily reuse and extend it if we encounter more repetitive code patterns in future work. 

I know "we" is just me and my wife right now. 


```{r}
#| warning: false

library(tidyverse)
library(nycflights13)
library(flightanalytics) # hey look thats new
library(ks) # this is new too


june27_flights <- flights |> 
  mutate(dep_dttm = time_hour + minutes(minute)) |> 
  filter(day == 27 , month == 6)


arrival_curve <- get_pgds_arrival_curve() |> 
  clean_arrival_curve()


pax_bag_data <- get_pgds_passenger_bag_factors()

```


### Recap of Arrival curve

Here's a summary of the arrival curves from the [previous post](https://www.ryanplain.com/posts/2024-06-01-flight-data/), which show the distribution using a continuous custom distribution:



```{r}

set.seed(0527)

arrival_samples <- arrival_curve |> 
  group_by(name) |> 
  nest() |> 
  ungroup() |> 
  mutate(
    samples = map(.x = data, ~sample(.x$minutes_prior, 1000, replace = TRUE, prob = .x$value))
  ) |> 
  mutate(
    d = map(samples, ~density(.x)),
    density_x = map(d, "x"),
    density_y = map(d, "y"),
    dens = tibble(dens_x = map(d, "x"), dens_y = map(d, "y"))
  ) |> 
  ungroup() 

arrival_kde <- arrival_samples |> 
  select(name, starts_with('density')) |> 
  unnest(c(density_x, density_y)) |> 
  filter(density_x > 0) |> 
  group_by(name) |> 
  mutate(y = density_y / sum(density_y)) |>
  select(name, minutes_prior = density_x, perc = y) |> 
  ungroup()

arrival_kde |> 
  ggplot(aes(minutes_prior, perc, color = name)) +
  geom_line(linewidth = 2) + 
  labs(
    title = 'KDE of Arrival Curve',
    x = 'Minutes Prior',
    y = NULL,
    color = NULL,
    fill = NULL,
  ) +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_x_reverse() + 
  theme(legend.position = 'top')


```


### What's a KDE?

One thing we touched on last time was using the Kernel Density Estimate (KDE). If you have every used `plot(density(x))`, you have used a KDE. You can learn more about it [here](https://en.wikipedia.org/wiki/Kernel_density_estimation), but to summarize it is a smoothing function to estimate the probability density function of a random variable.

::: {.callout-note collapse="true"}
## Make it Simple

I failed Algebra I multiple times, and I still have trauma from Greek letters in grad school. I was someone who learned math and statistics more effectively through programming. It's not for everyone (the way math is traditionally taught also isn't for everyone), but I found everything easier to learn inside a for loop. Colors help too. 

I'll link to Wikipedia and other sources to detail the math used, because that is important too. Just learn however is best for you, and it doesn't have to be how everyone else does it. If I add context it will be to make it as simple to grasp as possible, and I will never use the Greek alphabet.  
:::


### A quick example

In this example, 100 values are sampled from a normal distribution. However, due to the relatively small sample size, the resulting distribution may not precisely match the true normal distribution. Interestingly, I set my anniversary date as the random seed for reproducibility, and it unexpectedly yielded a bimodal density in the sample. The flexibility of the smoothing function is what allows us to match the unique arrival curve shapes. 

```{r}
#| warning: false

# Generate example data
set.seed(0527)  # For reproducibility
data <- data.frame(value = rnorm(100, mean = 5, sd = 2))

# Plot the histogram with density line
ggplot(data, aes(x = value)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 0.5, fill = "blue", alpha = 0.5) +
  geom_density(color = "red", linewidth = 1) +
  labs(title = "Histogram with Density Line",
       x = "Value",
       y = "Density") +
  theme_minimal()

```



### Create Base Flight Schedule

At the moment, this code is some `dplyr` mess that should be wrapped into a function to clean the flight schedule into a usable format. The main takeaway is that we have a flight schedule with the estimated passengers and bags for each flight.

```{r}


airports <- airports |> 
  mutate(domestic = str_detect(tzone, 'America'),
         domestic = replace_na(domestic, FALSE))



 arrival_curve_kde <- arrival_curve |> 
    group_by(name) |> 
    nest() |> 
    ungroup() |> 
    # sample 1000 values from 10 minute increments
    mutate(
      samples = map(.x = data, ~sample(.x$minutes_prior, 1000, replace = TRUE, prob = .x$value))
    ) |> 
   # apply KDE function, here {ks} is used which was different than the 
   # last post that used `d = map(samples, ~density(.x))`,
    mutate(
      .kde = map(.x = samples, ~ks::kde(.x)),
    ) |> 
    mutate(
      peak = name == 'peak_domestic_8am',
      domestic = !str_detect(name, 'international')
      )



june_27_base <- june27_flights |> 
  # join all the tables
  left_join(planes |> select(tailnum, seats), by = c('tailnum')) |> 
  left_join(pax_bag_data, by = c('carrier')) |> 
  left_join(airports, by =c('dest' = 'faa')) |> 
  # Some stations did not have a timezone, default to non-domestic
  mutate(
    domestic = replace_na(domestic, FALSE),
    join_arr_col = case_when(
      domestic == FALSE ~ 'international',
      sched_dep_time <= 800 ~ 'peak_domestic_8am',
      TRUE ~ 'off_peak_domestic'
    ),
    # use median values for missing airline data
    across(c(contains('factor'), avg_num_bags ), \(x) replace_na(x, median(x, na.rm = TRUE)))
  ) |> 
  # Fill in the NA seat values for small amount of missing data in planes table
  group_by(carrier) |> 
  mutate(seats = replace_na(seats, median(seats, na.rm = TRUE))) |> 
  ungroup() |> 
  # Apply factors to seats to get passengers and bags
  mutate(
    passengers = round(seats * load_factor),
    passengers_with_bag = round(seats * check_bag_factor),
    num_of_bags = round(passengers * avg_num_bags)
  ) |> 
  left_join(
    arrival_curve_kde,
      by = c('join_arr_col' = 'name'),
    suffix = c('_base', '_arr_curve')
    ) |> 
    select(carrier, origin, dest, dep_dttm, passengers:num_of_bags, arr_curve = join_arr_col, .kde)

june_27_base |> 
  head()
```


# Sampling from distribution

Previously, we applied a flat factor across the 10-minute arrival curve. Now, we're adding some variance to enhance our modeling.

Each flight now has one of the three different types of arrival curves associated with it. With the number of expected passengers, we want to have a value of how many minutes prior to departure they arrive. To implement this, we're utilizing `purrr` to apply the function `ks::rkde` to each row, adjusting for the number of passengers dynamically.

This approach enables us to introduce variability into our arrival curve modeling, enhancing the fidelity of our simulations and better capturing real-world scenarios.

```{r}
set.seed(1234)
# Generate random arrivals from the KDE for every passenger expected
june_27_kde_1 <- june_27_base |> 
  mutate(
    arrivals = map2(.x = .kde, .y = passengers,  ~ks::rkde(.y, .x))
  ) 

june_27_kde_1 |> 
  head()

june_27_kde_1 |> 
  unnest(arrivals) |> 
  ggplot(aes(arrivals, fill = arr_curve)) + 
  geom_histogram(bins=50) + 
  facet_wrap(~arr_curve, ncol = 1, scales = 'free_y') +
  scale_x_reverse() +
  theme(
    legend.position = 'none'
  ) +
  labs(
    title = 'Sampled Arrivals from Arrival Curve',
    subtitle = 'Note: the y axis are on different scales due to the disproportionate amount of domestic flights',
    x = 'Minutes Prior to Departure',
    y = 'Count'
  )
```

It matches the arrival curves from the plot above! 

Taking those arrivals, we can generate an timestamp when each passenger arrives. The plot below is the aggregate number of passengers arriving by minute over the course of the day. 


```{r}

# This is a common procedure I have followed before
# Get a list of values for each flight, 
# Unnest to create 1 row per passenger
# Subtract the minutes prior sampled from arrival curve, from the departure tiem
june_27_arrivals_long <- june_27_kde_1 |> 
  unnest(arrivals) |> 
  mutate(model_dttm = dep_dttm - minutes(round(arrivals))) 
  
june_27_arrivals_long |> 
  head()

june_27_arrivals_long |> 
  count(origin, model_dttm) |> 
  ggplot(aes(model_dttm, n, color = origin)) + 
  geom_line() + 
  facet_wrap(~origin, ncol = 1) +
  theme(
    legend.position = 'none'
  ) +
  labs(
    title = 'Passenger Arrivals Throughout the Day',
    x = 'Minutes Prior to Departure',
    y = 'Count'
  )

```

This is great, it is just one scenario thought. 

### Journey to 10k

I often see statements like "this analysis was based on 10,000 simulations," but if your underlying function doesn't accurately reflect the real world, no amount of simulations will compensate for that. The answer to how many simulations you need is, as always, "it depends."

What is the goal or intended outcome of your model? If you're focusing on extreme tail-end outcomes, 1,000 simulations might not be sufficient. Even 100,000 simulations might fall short if your distribution doesn't accurately represent reality.

A larger number of simulations becomes critical as we incorporate other metrics, like load and bag factors, into the model. Currently, we're simulating a single metric, and 1,000 simulations might be reasonable for this. The arrival curve is a well-studied concept, and we have reasonable confidence that the distribution is accurately representing the real world now. If it wasn't, we would want to look at expanding the range of the distribution.  

### Replicating the sampling over 10,000 simulations

No magic number in 10,000, we just want more than 1. 

#### Steps

1. Take the flight schedule and create a number of simulations that you want to do
2. Replicate each flight for the number of sims, this example uses `tidyr::unnest()` of a list column
3. Apply the sampling function for each passenger
4. Unnest the arrival column to represent 1 row for every passenger, in every simulation


```{r}

SIMS <- 10

june_27_kde_100 <- june_27_base |> 
  mutate(sim_number = list(1:SIMS)) |> 
  unnest(sim_number) |> 
  mutate(
    arrivals = map2(.x = .kde, .y = passengers,  ~ks::rkde(.y, .x))
  ) |> 
  unnest(arrivals)

june_27_kde_100 |> 
  mutate(model_dttm = dep_dttm - minutes(round(arrivals))) |> 
  count(origin, sim_number, model_dttm) |> 
  ggplot(aes(model_dttm, n)) + 
  geom_point(aes(group = sim_number), alpha = 0.5, color = 'grey') + 
  geom_point(
    aes(color = origin),
    alpha = 0.7 , 
    data =  june_27_arrivals_long |> count(origin, model_dttm)
    ) +
  facet_wrap(~origin, ncol = 1) +
  theme(
    legend.position = 'none'
  ) +
  labs(
    title = 'Passenger Arrivals Throughout the Day',
    x = 'Minutes Prior to Departure',
    y = 'Count'
  )


```

For each station, we have a single simulation plotted in color on top of all of the simulations. The overall curve does not change that much, as that would be determined more on simulating number of passengers and overall flight counts. However, adding a distribution to when passengers arrive introduces a significant amount of variance.

The order that the inputs would impact the overall passenger and bag profile is: 

1. **Flight Schedule:** Seat capacities and distribution of flights
2. **Load Factors:** How many of those seats are occupied
3. **Local Factor:** How many of those occupied seats are local
4. **Arrival**: When do the local occupied seats arrive at the airport

### Why Are We Doing It Backwards?

While the flight distribution has the biggest impact, it also changes infrequently. Human behavior and the random factors associated with arrivals change constantly. If we are planning for a specific flight schedule, we should at a minimum account for all arrival distributions.


But wait...

```{r}

paste("Number of rows:", nrow(june_27_kde_100))

```

Using only 100 simulations created 11.6M rows. Yikes! 

Getting to 1,000 simulations seems daunting, much less 10,000. Using dataframes isn't the only solution to manage this data. However, since they act as interfaces for handling related vectors, dataframes are by far the easiest tool for a typical analyst. For example, calculating the time of day a passenger arrives by subtracting one column from another datetime column is so easy with dataframes an AI bot could do it.

The tradeoff for this convenience is the memory and data capacity it consumes. Imagine running a web application with adjustable sliders for flight counts at a station to predict wait times at a checkpoint. If you're using R dataframes and dplyr, you'd better have plenty of RAM—and a lunch break planned for after you interact with the application.

### Do I have an answer? 

Not yet, but I'm gonna blog my way through it. 

#### Possible solutions

1. DuckDB 
2. Writing low level operations in something like Rust with `extendr`
3. `DataTable`, `Polars`, or other optimized dataframe libraries










































