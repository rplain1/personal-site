{
  "hash": "719ba05a019544b8072d6c95b1f0558a",
  "result": {
    "markdown": "---\ntitle: \"Arrival Curve Sampling\"\nauthor: \"Ryan Plain\"\ndate: \"2024-06-07\"\ncategories: [Flights, R]\ndescription: \"Adding variance using the KDE of each arrival curve\"\ndraft: false\nformat: \n  html: \n    code-fold: true\n    code-summary: \"Show the code\"\n\n---\n\n\n\n## Flight Analytics\n\nIn this post, we continue our exploration of modeling passenger and baggage traffic in airports using publicly available data. To streamline our data processing, I created a simple R package [flightanalytics](https://github.com/rplain1/flightanalytics). This package helps to build dataframes from the [Planning Guidelines and Design Standards for TSA](https://iabsc.org/wp-content/uploads/2021/04/Planning-Guidelines-and-Design-Standards-for-Checked-Baggage-Inspection-Systems-V7.0.pdf) study. \n\nFor now, we will use the arrival curve and some carrier-level aggregated passenger and bag data from the TSA document. The `flightanalytics` package is used for storing the logic required to build these tables from the PDF document. By consolidating this logic into a package, we can easily reuse and extend it if we encounter more repetitive code patterns in future work. \n\nI know \"we\" is just me and my wife right now. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(nycflights13)\nlibrary(flightanalytics) # hey look thats new\nlibrary(ks) # this is new too\n\npax_bag_data <- get_pgds_passenger_bag_factors()\n\narrival_curve <- get_pgds_arrival_curve() |> \n  clean_arrival_curve() |> \n  create_arrival_curve_kde()\n\nflight_schedule <- clean_and_join_nycflights()\n\n\npax_bag_data <- get_pgds_passenger_bag_factors()\n```\n:::\n\n\n\n### Recap of Arrival curve\n\nHere's a summary of the arrival curves from the [previous post](https://www.ryanplain.com/posts/2024-06-01-flight-data/), which show the distribution using a continuous custom distribution:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(0527)\n\narrival_samples <- get_pgds_arrival_curve() |> \n  clean_arrival_curve() |> \n  group_by(name) |> \n  nest() |> \n  ungroup() |> \n  mutate(\n    samples = map(.x = data, ~sample(.x$minutes_prior, 1000, replace = TRUE, prob = .x$value))\n  ) |> \n  mutate(\n    d = map(samples, ~density(.x)),\n    density_x = map(d, \"x\"),\n    density_y = map(d, \"y\"),\n    dens = tibble(dens_x = map(d, \"x\"), dens_y = map(d, \"y\"))\n  ) |> \n  select(name, starts_with('density')) |> \n  unnest(c(density_x, density_y)) |> \n  filter(density_x > 0) |> \n  group_by(name) |> \n  mutate(y = density_y / sum(density_y)) |>\n  select(name, minutes_prior = density_x, perc = y) |> \n  ungroup()\n\narrival_samples |> \n  ggplot(aes(minutes_prior, perc, color = name)) +\n  geom_line(linewidth = 2) + \n  labs(\n    title = 'KDE of Arrival Curve',\n    x = 'Minutes Prior',\n    y = NULL,\n    color = NULL,\n    fill = NULL,\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_x_reverse() + \n  theme(legend.position = 'top')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n### What's a KDE?\n\nOne thing we touched on last time was using the KDE (Kernel Density Estimate). If you have ever used `plot(density(x))`, the you have effectively used a KDE. You can learn more about it [here](https://en.wikipedia.org/wiki/Kernel_density_estimation), but to summarize it is a smoothing function to estimate the probability density function of a random variable.\n\n### A quick example\n\nIn this example, 100 values are sampled from a normal distribution. However, due to the relatively small sample size, the resulting distribution may not precisely match the true normal distribution. Interestingly, I set my anniversary date as the random seed for reproducibility, and it unexpectedly yielded a bimodal density in the sample. The flexibility of the smoothing function is what allows us to match the unique arrival curve shapes. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate example data\nset.seed(0527)  # For reproducibility\ndata <- data.frame(value = rnorm(100, mean = 5, sd = 2))\n\n# Plot the histogram with density line\nggplot(data, aes(x = value)) +\n  geom_histogram(aes(y = after_stat(density)), binwidth = 0.5, fill = \"blue\", alpha = 0.5) +\n  geom_density(color = \"red\", linewidth = 1) +\n  labs(title = \"Histogram with Density Line\",\n       x = \"Value\",\n       y = \"Density\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n### Sampling from distribution\n\nPreviously, we applied a flat factor across the 10-minute arrival curve. Now, we're adding some variance to enhance our modeling.\n\nEach flight now has one of the three different types of arrival curves associated with it. With the number of expected passengers, we want to have random values for each passenger representing how many minutes prior to departure they arrive. To implement this, we're utilizing `purrr` to apply the function `ks::rkde` to each row, adjusting for the number of passengers dynamically.\n\nThis approach enables us to introduce variability into our arrival curve modeling, enhancing the simulation and better capturing real-world scenarios.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njune_27_base <- flight_schedule |>\n  filter(month == 6, day == 27) |> \n  # join all the tables\n  left_join(pax_bag_data, by = c('carrier')) |> \n  mutate(\n  # use median values for missing airline data\n    across(c(contains('factor'), avg_num_bags ), \\(x) replace_na(x, median(x, na.rm = TRUE)))\n  ) |> \n  # Apply factors to seats to get passengers and bags\n  mutate(\n    passengers = round(seats * load_factor),\n    passengers_with_bag = round(seats * check_bag_factor),\n    num_of_bags = round(passengers * avg_num_bags)\n  ) |> \n  left_join(\n    arrival_curve,\n    by = c('flight_type' = 'name'),\n    suffix = c('_base', '_arr_curve')\n  ) |> \n  select(carrier, origin, dest, dep_dttm, flight_type, seats, passengers, avg_num_bags, .kde)\n\n\njune_27_base |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 9\n  carrier origin dest  dep_dttm            flight_type       seats passengers\n  <chr>   <chr>  <chr> <dttm>              <chr>             <dbl>      <dbl>\n1 UA      EWR    MIA   2013-06-27 20:10:00 off_peak_domestic   149        127\n2 B6      JFK    MCO   2013-06-27 21:46:00 off_peak_domestic    20         18\n3 B6      JFK    FLL   2013-06-27 21:55:00 off_peak_domestic   200        180\n4 B6      JFK    BUF   2013-06-27 21:10:00 off_peak_domestic    20         18\n5 UA      EWR    CLE   2013-06-27 20:21:00 off_peak_domestic   149        127\n6 B6      JFK    BOS   2013-06-27 23:00:00 off_peak_domestic    20         18\n# ℹ 2 more variables: avg_num_bags <dbl>, .kde <list>\n```\n:::\n:::\n\n\n### Distributions \n\nExamining the output of a single simulation, we observe that the distribution aligns with our expectations based on the KDE. Due to the fewer number of international flights in the schedule, the histogram appears less smooth compared to that of domestic flights. Which is exactly why we want to replicate this. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n# Generate random arrivals from the KDE for every passenger expected\njune_27_kde_1 <- june_27_base |> \n  mutate(\n    arrivals = map2(.x = .kde, .y = passengers,  ~ks::rkde(.y, .x))\n  ) \n\njune_27_kde_1 |> \n  unnest(arrivals) |> \n  ggplot(aes(arrivals, fill = flight_type)) + \n  geom_histogram(bins=50) + \n  facet_wrap(~flight_type, ncol = 1, scales = 'free_y') +\n  scale_x_reverse() +\n  theme(\n    legend.position = 'none'\n  ) +\n  labs(\n    title = 'Sampled Arrivals from Arrival Curve',\n    subtitle = 'Note: the y axis are on different scales due to the disproportionate amount of domestic flights',\n    x = 'Minutes Prior to Departure',\n    y = 'Count'\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### One Flight, Several Distirbutions\n\nn this example, we examine a single flight and draw arrival times from the arrival curve three independent times. Each resulting distribution differs, and when we aggregate the data across all flights and simulations throughout the day, it significantly impacts the number of passengers arriving at any given time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n# Generate random arrivals from the KDE for every passenger expected\n\njune_27_kde_1 |> \n  filter(carrier == 'UA', origin == 'EWR', dest == 'MIA', hour(dep_dttm) == 20) |> \n  mutate(rn = list(paste(\"sim\",1:3))) |> \n  unnest(rn) |> \n  mutate(arrivals = map2(.x = .kde, .y = passengers,  ~ks::rkde(.y, .x))) |> \n  unnest(arrivals) |> \n  ggplot(aes(arrivals, fill = rn)) + \n  geom_histogram(bins=50) + \n  facet_wrap(~rn, ncol = 1, scales = 'free_y') +\n  scale_x_reverse() +\n  theme(\n    legend.position = 'none'\n  ) +\n  labs(\n    title = 'Sampled Arrivals from Arrival Curve',\n    subtitle = 'Note: the y axis are on different scales due to the disproportionate amount of domestic flights',\n    x = 'Minutes Prior to Departure',\n    y = 'Count'\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### Modeling The Day\n\nAs we did in the previous post, we can take the time of arrival and derive what time of day to expect them. The following plot is the aggregate of all the arrivals based on one simulation. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This is a common procedure I have followed before\n# Get a list of values for each flight, \n# Unnest to create 1 row per passenger\n# Subtract the minutes prior sampled from arrival curve, from the departure tiem\njune_27_arrivals_long <- june_27_kde_1 |> \n  unnest(arrivals) |> \n  mutate(model_dttm = dep_dttm - minutes(round(arrivals))) \n  \njune_27_arrivals_long |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 11\n  carrier origin dest  dep_dttm            flight_type       seats passengers\n  <chr>   <chr>  <chr> <dttm>              <chr>             <dbl>      <dbl>\n1 UA      EWR    MIA   2013-06-27 20:10:00 off_peak_domestic   149        127\n2 UA      EWR    MIA   2013-06-27 20:10:00 off_peak_domestic   149        127\n3 UA      EWR    MIA   2013-06-27 20:10:00 off_peak_domestic   149        127\n4 UA      EWR    MIA   2013-06-27 20:10:00 off_peak_domestic   149        127\n5 UA      EWR    MIA   2013-06-27 20:10:00 off_peak_domestic   149        127\n6 UA      EWR    MIA   2013-06-27 20:10:00 off_peak_domestic   149        127\n# ℹ 4 more variables: avg_num_bags <dbl>, .kde <list>, arrivals <dbl>,\n#   model_dttm <dttm>\n```\n:::\n\n```{.r .cell-code}\njune_27_arrivals_long |> \n  count(origin, model_dttm) |> \n  ggplot(aes(model_dttm, n, color = origin)) + \n  geom_line() + \n  facet_wrap(~origin, ncol = 1) +\n  theme(\n    legend.position = 'none'\n  ) +\n  labs(\n    title = 'Passenger Arrivals Throughout the Day',\n    x = 'Time',\n    y = 'Count'\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThis is great, it is just one scenario thought. \n\n### Journey to 10k\n\nI often encounter statements like \"this analysis was based on 10,000 simulations,\" but if your underlying function doesn't accurately reflect the real world, no number of simulations will compensate for that. The answer to how many simulations you need is, as always, \"it depends.\"\n\nWhat is the goal or intended outcome of your model? If you're focusing on extreme tail-end outcomes, 1,000 simulations might not be sufficient. Even 100,000 simulations might fall short if your distribution doesn't accurately represent reality.\n\nA larger number of simulations becomes critical as we incorporate other metrics, such as load and bag factors, into the model. Currently, we're simulating a single metric, and 1,000 simulations might be reasonable for this. The arrival curve is a well-studied concept, and we have reasonable confidence that the distribution accurately represents the real world. If it didn’t, we would need to consider expanding the range of the distribution.\n\n### Replicating the sampling over 100 simulations \n\nYou will see why not 10,000, yet. \n\n#### Steps\n\n1. Take the flight schedule and create a number of simulations that you want to do\n2. Replicate each flight for the number of sims, this example uses `tidyr::unnest()` of a list column\n3. Apply the sampling function for each passenger\n4. Unnest the arrival column to represent 1 row for every passenger, in every simulation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSIMS <- 100\n\njune_27_kde_100 <- june_27_base |> \n  mutate(sim_number = list(1:SIMS)) |> \n  unnest(sim_number) |> \n  mutate(\n    arrivals = map2(.x = .kde, .y = passengers,  ~ks::rkde(.y, .x))\n  ) |> \n  unnest(arrivals)\n\njune_27_kde_100 |> \n  mutate(model_dttm = dep_dttm - minutes(round(arrivals))) |> \n  count(origin, sim_number, model_dttm) |> \n  ggplot(aes(model_dttm, n)) + \n  geom_point(aes(group = sim_number), alpha = 0.5, color = 'grey') + \n  geom_line(\n    aes(color = origin),\n    alpha = 0.7 , \n    data =  june_27_arrivals_long |> count(origin, model_dttm)\n    ) +\n  facet_wrap(~origin, ncol = 1) +\n  theme(\n    legend.position = 'none'\n  ) +\n  labs(\n    title = 'Passenger Arrivals Throughout the Day',\n    x = 'Time',\n    y = 'Count'\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nFor each station, we have the previous single simulation plotted in color on top of all of the simulations. The overall curve does not change that much, as that would be determined more on simulating number of passengers and overall flight counts. However, adding a distribution to when passengers arrive introduces a significant amount of variance not captured with the single interation.\n\nThe order that the inputs would impact the overall passenger and bag profile is: \n\n1. **Flight Schedule:** Seat capacities and distribution of flights\n2. **Load Factors:** How many of those seats are occupied\n3. **Local Factor:** How many of those occupied seats are local\n4. **Arrival**: When do the local occupied seats arrive at the airport\n\n### Why Are We Doing It Backwards?\n\nWhile the flight distribution has the biggest impact, it also changes infrequently. Human behavior and the random factors associated with arrivals change constantly. If we are planning for a specific flight schedule, we should at a minimum account for all arrival distributions.\n\n\nBut wait...\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaste(\"Number of rows:\", nrow(june_27_kde_100))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Number of rows: 11595600\"\n```\n:::\n:::\n\n\nUsing only 100 simulations created 11.6M rows. Yikes! \n\nGetting to 1,000 simulations seems daunting, let alone 10,000. Using dataframes aren't the only solution for managing this data, they are by far the easiest tool for a typical analyst due to their ability to handle related vectors seamlessly. For example, calculating the time of day a passenger arrives by subtracting one column from another datetime column is so straightforward with dataframes that even an AI bot could do it.\n\nThe tradeoff for this convenience is the memory and data capacity it consumes. Imagine running a web application with adjustable sliders for flight counts at a station to predict wait times at a checkpoint. If you're using R dataframes and `dplyr`, you'd better have plenty of RAM—and a lunch break planned for after you interact with the application.\n\n### DuckDB\n\nDuckDB is a technology I have recently been getting into. I am continuously impressed the deeper I dive into it. At the time of writing this, they recently released [version 1.0](https://duckdb.org/2024/06/03/announcing-duckdb-100.html). I'm really excited for its future and all the potential use cases. Other projects like [Modern Data Stack In a Box](https://mdsinabox.com/) have a full infrastructure of models and visualizations powered by DuckDB (along with dbt and evidence).\n\nI've yet to write about DuckDB, and the scope of this isn't necessarily to teach about it, but we will use it as a tool. In a high level overview, it operates as an in-process analytical database management system. It is analytical due to its columnar oriented structure and optimized to analyze data. \n\n### Dataframes\n\nI love R dataframes (okay I really love tibbles), they are quick and easy to get started analyzing data. The `{tidyverse}` is a great way to filter, aggregate, and visualize data. As the data increases, there are dimminishing returns on the speed and ease of use with the amount of memory the dataframe object takes up causes everything to slow down. If you are working in Rstudio, you might have seen the bomb 💣 causing you to have to restart your entire session. \n\nWhat size and tradeoff to leverage dataframes or not is dependent on your use case and hardware. I'm working on a laptop with 16gb of memory, and a couple of GB of data will bog my system down. I'm going to leverage DuckDB to work with dataframes that I otherwise could not in a typical R session on my machine. \n\n\n### DuckDB Setup\n\nWe could use DuckDB in-memory, and not need to utilize any path information, but to work with out-of-memory data we will need to create a `.duckdb` file. The following code will just check if the directory exists, if not create it and connect. \n\nThis will create 2 files:\n\n- `sim_dataset.duckdb`\n- `sim_dataset.duckdb.wal`\n\nThe `sim_dataset.duckdb` file will be the file used for the database connection. The `sim_dataset.duckdb.wal` stands for write-ahead log (WAL). THE WAL file is used to log changes before executed in the main database file, and helps with recovering from crashes and maintaining transactions. This use case won't rely too much on it, as we will create a temporary table, but I have not seen anywhere that it would be safe to delete it. If it is anything like [SQLite] (https://stackoverflow.com/questions/20969996/is-it-safe-to-delete-sqlites-wal-file), I would avoid messing with it. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n# setup\nlibrary(duckdb)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: DBI\n```\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\ncurrent_directory <- getwd()\n\n# Define the name of the new directory\nnew_directory <- \"posts/2024-06-02-flight-data-cont\"\nnew_directory <- 'tmp'\n\n# Combine the current working directory with the new directory name\nnew_directory_path <- file.path(current_directory, new_directory)\n\n# Check if the directory exists\nif (!dir.exists(new_directory_path)) {\n  # Create the new directory if it does not exist\n  dir.create(new_directory_path)\n  print(paste(\"New directory created at:\", new_directory_path))\n} else {\n  print(paste(\"Directory already exists at:\", new_directory_path))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Directory already exists at: /Users/ryan/git-repos/personal-site/posts/2024-06-02-flight-data-cont/tmp\"\n```\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\n#con <- dbConnect(duckdb::duckdb(), \"posts/2024-06-02-flight-data-cont/sim_dataset.duckdb\")\ncon <- dbConnect(duckdb::duckdb(), \"tmp/sim_dataset_tmp.duckdb\")\n```\n:::\n\n\n### Simulate Data\n\nWith our DuckDB database file set up, we can now run simulations. The following code chunks the operations of the simulation and writes/appends the results to the table in DuckDB. We continue to leverage R dataframes and use `purrr::map2()` for sampling from the KDE. To avoid memory constraints that could potentially crash the system, we ensure the R dataframe does not become too large.\n\n\n::: {.cell}\n\n:::\n\n\n### DuckDB Table - Arrivals\n\nThe table now has each flight, simulated 1,000 times. Our base dataframe was 995 flights. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\ntbl(con, \"arrivals\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   table<arrivals> [?? x 8]\n# Database: DuckDB v0.10.2 [root@Darwin 23.4.0:R 4.3.2//Users/ryan/git-repos/personal-site/posts/2024-06-02-flight-data-cont/tmp/sim_dataset_tmp.duckdb]\n   carrier origin dest  dep_dttm            passengers avg_num_bags  sims\n   <chr>   <chr>  <chr> <dttm>                   <dbl>        <dbl> <int>\n 1 UA      EWR    MIA   2013-06-28 00:10:00        127         0.87     1\n 2 UA      EWR    MIA   2013-06-28 00:10:00        127         0.87     2\n 3 UA      EWR    MIA   2013-06-28 00:10:00        127         0.87     3\n 4 UA      EWR    MIA   2013-06-28 00:10:00        127         0.87     4\n 5 UA      EWR    MIA   2013-06-28 00:10:00        127         0.87     5\n 6 UA      EWR    MIA   2013-06-28 00:10:00        127         0.87     6\n 7 UA      EWR    MIA   2013-06-28 00:10:00        127         0.87     7\n 8 UA      EWR    MIA   2013-06-28 00:10:00        127         0.87     8\n 9 UA      EWR    MIA   2013-06-28 00:10:00        127         0.87     9\n10 UA      EWR    MIA   2013-06-28 00:10:00        127         0.87    10\n# ℹ more rows\n# ℹ 1 more variable: arrivals <list>\n```\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\ntbl(con, \"arrivals\") |> \n  count()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   SQL [1 x 1]\n# Database: DuckDB v0.10.2 [root@Darwin 23.4.0:R 4.3.2//Users/ryan/git-repos/personal-site/posts/2024-06-02-flight-data-cont/tmp/sim_dataset_tmp.duckdb]\n        n\n    <dbl>\n1 9950000\n```\n:::\n:::\n\n\n### Let's make it loooooong\n\nThe first output shows the row count when we `unnest()` the arrivals column, transforming the data to represent one row per passenger instead of per flight. This results in a significantly larger dataset, far beyond what I can handle in-memory on my machine.\n\nBoth table counts are printed to show that we are able to transform and aggregate the data **OUT-OF-MEMORY**. The potential DuckDB unlocks is absolutely insane. When I got started in 2019, I had a MacBook Air that had 8 GB of RAM (I still only have 16 GB 🙃), which limited the amount of data I could process. With DuckDB, we can leverage additional storage and CPUs to handle larger datasets efficiently.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\ntbl(con, \"arrivals\") |> \n  mutate(arrivals = sql(\"unnest(arrivals)\")) |> \n  count()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   SQL [1 x 1]\n# Database: DuckDB v0.10.2 [root@Darwin 23.4.0:R 4.3.2//Users/ryan/git-repos/personal-site/posts/2024-06-02-flight-data-cont/tmp/sim_dataset_tmp.duckdb]\n           n\n       <dbl>\n1 1159560000\n```\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\ntbl(con, \"arrivals\") |> \n  mutate(\n    arrivals = sql(\"unnest(arrivals)\"),\n    model_dttm = dep_dttm - minutes(arrivals)\n    ) |> \n  group_by(origin, model_dttm, sims) |> \n  summarise(\n    n = n(),\n    .groups = 'drop'\n  ) |> \n  count()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   SQL [1 x 1]\n# Database: DuckDB v0.10.2 [root@Darwin 23.4.0:R 4.3.2//Users/ryan/git-repos/personal-site/posts/2024-06-02-flight-data-cont/tmp/sim_dataset_tmp.duckdb]\n         n\n     <dbl>\n1 34869398\n```\n:::\n:::\n\n\n# Results\n\nWith the simulations completed and the ability to transform and aggregate the data out-of-memory, let's examine the results. We’ve been analyzing the data on a minute-by-minute basis, but for the final output, we will group it into 5-minute intervals. This approach helps convey variance in a simpler and more intuitive manner.\n\nThe plot displays the average number of passengers, along with the minimum and maximum values from the simulations. The colored line represents the average number of passengers expected to arrive in 5-minute increments. The spread of the distribution illustrates the variance in the simulation.\n\nExamining the peaks for each station, we observe that between 200 to 400 passengers could arrive within any given 5-minute interval during that time. The uncertainty introduced by the arrival curve can significantly impact airport operations.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\ndf_arrivals <- tbl(con, \"arrivals\") |> \n  mutate(\n    arrivals = sql(\"unnest(arrivals)\"),\n    model_dttm = dep_dttm - minutes(arrivals),\n    model_dttm = model_dttm - minutes(minute(model_dttm) %% 5)\n    ) |> \n  count(origin, model_dttm, sims) |> \n  collect()\n\n\ndf_arrivals |> \n  group_by(origin, model_dttm) |> \n  mutate(\n    across(n, list(mean = mean, min=min, max=max))\n  ) |> \n  ggplot(aes(model_dttm, n)) +\n  geom_ribbon(aes(ymin = n_min, ymax=n_max), fill = 'grey70') + \n  geom_line(aes(y = n_mean, color = origin)) + \n  facet_wrap(~origin, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n### What's Next? \n\nWe can use the output of the arrivals to pass through Discrete Event Simulation software, such as [simmer](https://r-simmer.org/) or [SimPy](https://simpy.readthedocs.io/en/latest/). You can probably guess which route I will be utilizing.\n\nThere is also room for further optimization of the passenger arrivals data. As mentioned earlier, dataframes aren't necessarily the optimal approach. Columns like Carrier, Origin, Destination, and Departure Time are all repeated. The next step could involve using a NoSQL or document storage approach to handle these redundancies more efficiently.\n\nThe ultimate goal of this workflow is to power a Shiny app focused on a single station. The arrivals generated using this method would initialize relatively quickly, and with DuckDB, it should be sufficient to filter and collect the necessary data efficiently.\n\n:::{.callout-tip collapse=\"true\"}\n## Expand for Session Info\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       macOS Sonoma 14.4\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Denver\n date     2024-06-09\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package         * version date (UTC) lib source\n DBI             * 1.2.2   2024-02-16 [1] CRAN (R 4.3.1)\n dplyr           * 1.1.4   2023-11-17 [1] CRAN (R 4.3.1)\n duckdb          * 0.10.2  2024-05-01 [1] CRAN (R 4.3.1)\n flightanalytics * 0.1.0   2024-06-05 [1] Github (rplain1/flightanalytics@2ecda88)\n forcats         * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2         * 3.4.4   2023-10-12 [1] CRAN (R 4.3.1)\n ks              * 1.14.2  2024-01-15 [1] CRAN (R 4.3.1)\n lubridate       * 1.9.3   2023-09-27 [1] CRAN (R 4.3.1)\n nycflights13    * 1.0.2   2021-04-12 [1] CRAN (R 4.3.0)\n purrr           * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr           * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n sessioninfo     * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr         * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble          * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr           * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse       * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}